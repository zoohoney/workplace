{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd00af283f273ba8452268ed83ca0fd9f2c86ae0020769dca83ca1aff968103fc19",
   "display_name": "Python 3.7.10 64-bit ('aiffel': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 프로젝트 :ResNet Ablation Study"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1) ResNet 기본 블록 구성하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras & # Helper libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Tensorflow가 활용할 GPU가 장착되어 있는지 확인해 봅니다.\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "dataset_name = 'cats_vs_dogs'\n",
    "ds_train, ds_info_train = tfds.load(name=dataset_name, split='train[:80%]', as_supervised=True, with_info=True)\n",
    "ds_valid, ds_info_valid = tfds.load(name=dataset_name, split='train[80%:]', as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FeaturesDict({\n    'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n    'image/filename': Text(shape=(), dtype=tf.string),\n    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n})\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 데이터셋을 로드하면 꼭 feature 정보를 확인해 보세요. \n",
    "print(ds_info_train.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['cat', 'dog']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# label 확인\n",
    "ds_info_train.features[\"label\"].num_classes\n",
    "ds_info_valid.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=1\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1(x): \n",
    "    # Conv2D\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(7,7),\n",
    "        strides=(2,2),\n",
    "        kernel_initializer='he_normal',\n",
    "        padding='same', name='conv2d_7x7'\n",
    "    )(x)\n",
    "\n",
    "    # Batch Normalization \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    # Activation\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # maxpooling\n",
    "    x = keras.layers.MaxPooling2D(\n",
    "             pool_size=(2, 2),\n",
    "             strides=2,\n",
    "             name='stage2_0_maxpooling'\n",
    "            )(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "    # residual 통과(layer-18,34): 1*1 conv2d stride2 -> BN\n",
    "def conv_short(x,filters,stage_num,block_num):\n",
    "    \n",
    "    # Conv2D\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=(1,1),\n",
    "        kernel_initializer='he_normal',\n",
    "        padding='same', strides=(2,2),\n",
    "        name=f'stage{stage_num}_{block_num+1}_short'\n",
    "    )(x)\n",
    "    \n",
    "    # Batch Normalization \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "    # residual 통과(layer-50,101,152): 1*1 conv2d stride1 -> BN\n",
    "def conv_short50(x,filters, stage_num, block_num):\n",
    "    \n",
    "    # Conv2D\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=(1,1),\n",
    "        kernel_initializer='he_normal',\n",
    "        padding='same', strides=(1,1), \n",
    "        name=f'stage{stage_num}_{block_num+1}_short'\n",
    "    )(x)\n",
    "    \n",
    "    # Batch Normalization \n",
    "    x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn4')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "source": [
    "### 2) ResNet-34, ResNet-50 Complete Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_master1(x,\n",
    "                 first_layer,\n",
    "                 block_num,\n",
    "                 kernel_size,\n",
    "                 filters,\n",
    "                 stage_num,\n",
    "                 num_layer=34,\n",
    "                 is_plain=False\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    conv2\n",
    "    1) layer-18: [[3*3],64, [3*3],64]*2\n",
    "    2) layer-34: [[3*3],64, [3*3],64]*3\n",
    "    3) layer-50: [[1*1],64, [3*3],64, [1*1],256]\n",
    "    \"\"\"\n",
    "    \n",
    "    # residual 정의\n",
    "    residual = x\n",
    "    \n",
    "    if num_layer == 18 or num_layer == 34:\n",
    "        # Conv2D\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            kernel_initializer='he_normal',\n",
    "            padding='same', name=f'stage{stage_num}_{block_num+1}_conv1'\n",
    "        )(x)\n",
    "\n",
    "        # Batch Normalization \n",
    "        x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn1')(x)\n",
    "\n",
    "        # Activation\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "        # Conv2D\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            kernel_initializer='he_normal',\n",
    "            padding='same', name=f'stage{stage_num}_{block_num+1}_conv2'\n",
    "        )(x)\n",
    "\n",
    "        # Batch Normalization \n",
    "        x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn2')(x)\n",
    "        \n",
    "        if not is_plain:\n",
    "            # add\n",
    "            x = tf.keras.layers.Add(name=f'stage{stage_num}_{block_num+1}_add')([x, residual])\n",
    "\n",
    "        # activation\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    else:\n",
    "        # 3개 층 중에서 첫번째 층만 shortcut이고 나머지 두 개는 입력값을 그대로 사용\n",
    "        \n",
    "        # Conv2D\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size[0],\n",
    "            kernel_initializer='he_normal',\n",
    "            padding='same', name=f'stage{stage_num}_{block_num+1}_conv1'\n",
    "        )(x)\n",
    "        \n",
    "        # Batch Normalization \n",
    "        x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn1')(x)\n",
    "\n",
    "        # Activation\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "        # Conv2D\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size[1],\n",
    "            kernel_initializer='he_normal', name=f'stage{stage_num}_{block_num+1}_conv2',\n",
    "            padding='same'\n",
    "        )(x)\n",
    "\n",
    "        # Batch Normalization \n",
    "        x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn2')(x)\n",
    "\n",
    "        # Activation\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "        # Conv2D\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters=filters*4, # 마지막 레이어는 4배이다.\n",
    "            kernel_size=kernel_size[2],\n",
    "            kernel_initializer='he_normal', name=f'stage{stage_num}_{block_num+1}_conv3',\n",
    "            padding='same'\n",
    "        )(x)\n",
    "        \n",
    "        # short\n",
    "        conv3_short = conv_short50(residual, filters=filters*4, stage_num=stage_num, block_num=block_num)\n",
    "        \n",
    "        # Batch Normalization \n",
    "        x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn3')(x)\n",
    "        \n",
    "        if not is_plain:\n",
    "            if first_layer:\n",
    "                # add\n",
    "                x = tf.keras.layers.Add(name=f'stage{stage_num}_{block_num+1}_add')([x, conv3_short])\n",
    "            else:\n",
    "                # add\n",
    "                x = tf.keras.layers.Add(name=f'stage{stage_num}_{block_num+1}_add')([x, residual])\n",
    "            \n",
    "        # activation\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv3_x ~ conv5_x 블럭\n",
    "def conv_master2(x,\n",
    "                 block_num,\n",
    "                 kernel_size,\n",
    "                 filters,\n",
    "                 first_layer, # True/False,\n",
    "                 stage_num,\n",
    "                 num_layer=34,\n",
    "                 is_plain=False\n",
    "                 ):\n",
    "    \n",
    "    residual = x\n",
    "    \n",
    "    # kernel_size_copy\n",
    "    if num_layer == 50 or num_layer == 101 or num_layer == 152:\n",
    "        kernel_size_copy = kernel_size.copy()\n",
    "        kernel_size = kernel_size_copy[0]\n",
    "        \n",
    "    # <!-- first block --!>    \n",
    "    # 첫 번재 층은 stride를 해야 함.\n",
    "    if first_layer:\n",
    "        \n",
    "        # Conv2D\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            kernel_initializer='he_normal',\n",
    "            padding='same', strides=(2,2), # stride\n",
    "            name=f'stage{stage_num}_{block_num+1}_conv1'\n",
    "        )(x)\n",
    "    \n",
    "    else:\n",
    "        # Conv2D\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            kernel_initializer='he_normal',\n",
    "            padding='same',\n",
    "            name=f'stage{stage_num}_{block_num+1}_conv1'\n",
    "        )(x)\n",
    "    \n",
    "    # Batch Normalization \n",
    "    x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn1')(x)\n",
    "\n",
    "    # Activation\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    if num_layer==50 or num_layer==101 or num_layer==152: \n",
    "        # <!-- second block --!>\n",
    "        kernel_size = kernel_size_copy[1]\n",
    "#         print(f'stage{stage_num}_{block_num+1}_conv2')\n",
    "#         print('kernel_size: ', kernel_size)\n",
    "        # Conv2D\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            kernel_initializer='he_normal',name=f'stage{stage_num}_{block_num+1}_conv2',\n",
    "            padding='same'\n",
    "        )(x)\n",
    "\n",
    "        # Batch Normalization \n",
    "        x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn2')(x)\n",
    "\n",
    "        # Activation\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    # <!-- third block --!>\n",
    "#     print(f'stage{stage_num}_{block_num+1}_conv2')\n",
    "#     print('kernel_size: ', kernel_size)\n",
    "    conv_num=3\n",
    "    if num_layer==50 or num_layer==101 or num_layer==152:\n",
    "        filters=filters*4\n",
    "        kernel_size = kernel_size_copy[2]\n",
    "    else: conv_num-=1\n",
    "    \n",
    "    \n",
    "    # Conv2D\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        kernel_initializer='he_normal',name=f'stage{stage_num}_{block_num+1}_conv{conv_num}',\n",
    "        padding='same'\n",
    "    )(x)\n",
    "\n",
    "    # short\n",
    "    conv3_short = conv_short(residual, filters, stage_num=stage_num, block_num=block_num)\n",
    "    conv3_short50 = conv_short50(residual, filters, stage_num=stage_num, block_num=block_num)\n",
    "\n",
    "    # Batch Normalization \n",
    "    x = tf.keras.layers.BatchNormalization(name=f'stage{stage_num}_{block_num+1}_bn{conv_num}')(x)\n",
    "    \n",
    "    if not is_plain:\n",
    "        if first_layer:\n",
    "            # add\n",
    "            x = tf.keras.layers.Add(name=f'stage{stage_num}_{block_num+1}_add')([x, conv3_short])\n",
    "        else:\n",
    "            # add\n",
    "            x = tf.keras.layers.Add(name=f'stage{stage_num}_{block_num+1}_add')([x, residual])\n",
    "\n",
    "    # activation\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "source": [
    "#### building ResNet Block"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_layer, num_layer=34, is_residual=True, is_plain=False):\n",
    "    \n",
    "    # layer개수에 따른 cnn층 개수\n",
    "    cnn_layer ={\n",
    "                18 : [2,2,2,2], \n",
    "                34 : [3,4,6,3], \n",
    "                50 : [3,4,6,3], \n",
    "                101 : [3,4,23,3],\n",
    "                152 : [3,8,36,3]\n",
    "                }\n",
    "    \n",
    "    # layer개수에 따른 filter 리스트\n",
    "    filter_list = [64, 128, 256, 512]\n",
    "    \n",
    "    # layer별 커널 사이즈\n",
    "    kernel_size_dict = {18:(3,3),\n",
    "                   34:(3,3),\n",
    "                   50:[(1,1),(3,3),(1,1)],\n",
    "                   101:[(1,1),(3,3),(1,1)],\n",
    "                   152:[(1,1),(3,3),(1,1)]}\n",
    "    \n",
    "    # layer별 커널 사이즈\n",
    "    kernel_size = kernel_size_dict[num_layer]\n",
    "    \n",
    "    # cnn층 블록 개수 리스트(num_cnn_list) 할당\n",
    "    num_cnn_list = cnn_layer[num_layer]\n",
    "    \n",
    "    # 전체 conv 블록 개수\n",
    "    num_conv = len(num_cnn_list)\n",
    "    \n",
    "    # 입력 레이어\n",
    "    x = input_layer\n",
    "    \n",
    "    # 7*7, 64, stride2 + maxpooling\n",
    "    x = conv1(x)\n",
    "    \n",
    "    # stage num 정의\n",
    "    stage_num = 2\n",
    "    \n",
    "    # conv_idx 정의\n",
    "    conv_idx=0\n",
    "\n",
    "    # conv2_x\n",
    "    \n",
    "    # layer-50,101,152만 해당\n",
    "    first_layer = [True,False,False]\n",
    "    \n",
    "    for i in range(num_cnn_list[conv_idx]):\n",
    "        x = conv_master1(x,\n",
    "                         first_layer=first_layer[i],\n",
    "                         block_num=i,\n",
    "                         kernel_size=kernel_size,\n",
    "                         filters=filter_list[conv_idx],\n",
    "                         stage_num=stage_num,\n",
    "                         num_layer=num_layer,\n",
    "                         is_plain=is_plain\n",
    "                         )\n",
    "#     print('conv2 완료')\n",
    "\n",
    "    # conv3_x\n",
    "    stage_num+=1\n",
    "    conv_idx+=1\n",
    "    if is_residual: # residual 있을때\n",
    "        first_layer = [True]\n",
    "        for _ in range(num_cnn_list[conv_idx]-1): first_layer.extend([False])\n",
    "    else:          # residual 없을때\n",
    "        first_layer = [False]*num_cnn_list[conv_idx]\n",
    "    \n",
    "    for i in range(num_cnn_list[conv_idx]):\n",
    "        x = conv_master2(x,\n",
    "                        block_num=i,\n",
    "                        kernel_size=kernel_size,\n",
    "                        filters=filter_list[conv_idx],\n",
    "                        first_layer=first_layer[i],\n",
    "                        stage_num=stage_num,\n",
    "                        num_layer=num_layer,\n",
    "                        is_plain=is_plain)\n",
    "        \n",
    "#     print('conv3 완료')\n",
    "\n",
    "    # conv4_x\n",
    "    stage_num+=1\n",
    "    conv_idx+=1\n",
    "    \n",
    "    if is_residual:\n",
    "        first_layer = [True]\n",
    "        for _ in range(num_cnn_list[conv_idx]-1): first_layer.extend([False])\n",
    "    else:\n",
    "        first_layer = [False]*num_cnn_list[conv_idx]\n",
    "#     print('first_layer: ',first_layer)\n",
    "    for i in range(num_cnn_list[conv_idx]):\n",
    "        x = conv_master2(x,\n",
    "                        block_num=i,\n",
    "                        kernel_size=kernel_size,\n",
    "                        filters=filter_list[conv_idx],\n",
    "                        first_layer=first_layer[i],\n",
    "                        stage_num=stage_num,\n",
    "                        num_layer=num_layer,\n",
    "                        is_plain=is_plain)\n",
    "#     print('conv4 완료')\n",
    "    # conv5_x\n",
    "    stage_num+=1\n",
    "    conv_idx+=1\n",
    "    \n",
    "    if is_residual:\n",
    "        first_layer = [True]\n",
    "        for _ in range(num_cnn_list[conv_idx]-1): first_layer.extend([False])\n",
    "    else:\n",
    "        first_layer = [False]*num_cnn_list[conv_idx]\n",
    "    \n",
    "    for i in range(num_cnn_list[conv_idx]):\n",
    "        x = conv_master2(x,\n",
    "                        block_num=i,\n",
    "                        kernel_size=kernel_size,\n",
    "                        filters=filter_list[conv_idx],\n",
    "                        first_layer=first_layer[i],\n",
    "                        stage_num=stage_num,\n",
    "                        num_layer=num_layer,\n",
    "                        is_plain=is_plain)\n",
    "#     print('conv5 완료')\n",
    "\n",
    "    # avg pooling\n",
    "    x= keras.layers.AveragePooling2D(\n",
    "        pool_size=(2, 2), strides=2, padding='SAME', name='avg_pool')(x)\n",
    "    \n",
    "    # flatten\n",
    "    x = keras.layers.Flatten(name='flatten_11')(x)\n",
    "    \n",
    "    # FC layer(2) --- 우리가 사용할 데이터는 '고양이', '개' 두 가지 클래스만 있으므로 이걸 사용해야함.\n",
    "    x = keras.layers.Dense(2,name='fc2')(x)\n",
    "    \n",
    "    # FC layer(10) --- 노드의 파라미터 확인용(cifar-10일 때는 이것을 사용해야함) 확인이 끝난 후에는 다시 주석 처리했음.\n",
    "#     x = keras.layers.Dense(10,name='fc2')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3) ResNet-34, ResNet-50 Model 학습 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCH = 20\n",
    "\n",
    "# 데이터 정의\n",
    "ds_train = apply_normalize_on_dataset(ds_train, batch_size=BATCH_SIZE)\n",
    "ds_valid = apply_normalize_on_dataset(ds_valid, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_layer (InputLayer)        [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_7x7 (Conv2D)             (None, 112, 112, 64) 9472        input_layer[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_28 (BatchNo (None, 112, 112, 64) 256         conv2d_7x7[0][0]                 \n__________________________________________________________________________________________________\nactivation_82 (Activation)      (None, 112, 112, 64) 0           batch_normalization_28[0][0]     \n__________________________________________________________________________________________________\nstage2_0_maxpooling (MaxPooling (None, 56, 56, 64)   0           activation_82[0][0]              \n__________________________________________________________________________________________________\nstage2_1_conv1 (Conv2D)         (None, 56, 56, 64)   36928       stage2_0_maxpooling[0][0]        \n__________________________________________________________________________________________________\nstage2_1_bn1 (BatchNormalizatio (None, 56, 56, 64)   256         stage2_1_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_83 (Activation)      (None, 56, 56, 64)   0           stage2_1_bn1[0][0]               \n__________________________________________________________________________________________________\nstage2_1_conv2 (Conv2D)         (None, 56, 56, 64)   36928       activation_83[0][0]              \n__________________________________________________________________________________________________\nstage2_1_bn2 (BatchNormalizatio (None, 56, 56, 64)   256         stage2_1_conv2[0][0]             \n__________________________________________________________________________________________________\nstage2_1_add (Add)              (None, 56, 56, 64)   0           stage2_1_bn2[0][0]               \n                                                                 stage2_0_maxpooling[0][0]        \n__________________________________________________________________________________________________\nactivation_84 (Activation)      (None, 56, 56, 64)   0           stage2_1_add[0][0]               \n__________________________________________________________________________________________________\nstage2_2_conv1 (Conv2D)         (None, 56, 56, 64)   36928       activation_84[0][0]              \n__________________________________________________________________________________________________\nstage2_2_bn1 (BatchNormalizatio (None, 56, 56, 64)   256         stage2_2_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_85 (Activation)      (None, 56, 56, 64)   0           stage2_2_bn1[0][0]               \n__________________________________________________________________________________________________\nstage2_2_conv2 (Conv2D)         (None, 56, 56, 64)   36928       activation_85[0][0]              \n__________________________________________________________________________________________________\nstage2_2_bn2 (BatchNormalizatio (None, 56, 56, 64)   256         stage2_2_conv2[0][0]             \n__________________________________________________________________________________________________\nstage2_2_add (Add)              (None, 56, 56, 64)   0           stage2_2_bn2[0][0]               \n                                                                 activation_84[0][0]              \n__________________________________________________________________________________________________\nactivation_86 (Activation)      (None, 56, 56, 64)   0           stage2_2_add[0][0]               \n__________________________________________________________________________________________________\nstage2_3_conv1 (Conv2D)         (None, 56, 56, 64)   36928       activation_86[0][0]              \n__________________________________________________________________________________________________\nstage2_3_bn1 (BatchNormalizatio (None, 56, 56, 64)   256         stage2_3_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_87 (Activation)      (None, 56, 56, 64)   0           stage2_3_bn1[0][0]               \n__________________________________________________________________________________________________\nstage2_3_conv2 (Conv2D)         (None, 56, 56, 64)   36928       activation_87[0][0]              \n__________________________________________________________________________________________________\nstage2_3_bn2 (BatchNormalizatio (None, 56, 56, 64)   256         stage2_3_conv2[0][0]             \n__________________________________________________________________________________________________\nstage2_3_add (Add)              (None, 56, 56, 64)   0           stage2_3_bn2[0][0]               \n                                                                 activation_86[0][0]              \n__________________________________________________________________________________________________\nactivation_88 (Activation)      (None, 56, 56, 64)   0           stage2_3_add[0][0]               \n__________________________________________________________________________________________________\nstage3_1_conv1 (Conv2D)         (None, 28, 28, 128)  73856       activation_88[0][0]              \n__________________________________________________________________________________________________\nstage3_1_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_1_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_89 (Activation)      (None, 28, 28, 128)  0           stage3_1_bn1[0][0]               \n__________________________________________________________________________________________________\nstage3_1_conv2 (Conv2D)         (None, 28, 28, 128)  147584      activation_89[0][0]              \n__________________________________________________________________________________________________\nstage3_1_short (Conv2D)         (None, 28, 28, 128)  8320        activation_88[0][0]              \n__________________________________________________________________________________________________\nstage3_1_bn2 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_1_conv2[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_29 (BatchNo (None, 28, 28, 128)  512         stage3_1_short[0][0]             \n__________________________________________________________________________________________________\nstage3_1_add (Add)              (None, 28, 28, 128)  0           stage3_1_bn2[0][0]               \n                                                                 batch_normalization_29[0][0]     \n__________________________________________________________________________________________________\nactivation_90 (Activation)      (None, 28, 28, 128)  0           stage3_1_add[0][0]               \n__________________________________________________________________________________________________\nstage3_2_conv1 (Conv2D)         (None, 28, 28, 128)  147584      activation_90[0][0]              \n__________________________________________________________________________________________________\nstage3_2_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_2_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_91 (Activation)      (None, 28, 28, 128)  0           stage3_2_bn1[0][0]               \n__________________________________________________________________________________________________\nstage3_2_conv2 (Conv2D)         (None, 28, 28, 128)  147584      activation_91[0][0]              \n__________________________________________________________________________________________________\nstage3_2_bn2 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_2_conv2[0][0]             \n__________________________________________________________________________________________________\nstage3_2_add (Add)              (None, 28, 28, 128)  0           stage3_2_bn2[0][0]               \n                                                                 activation_90[0][0]              \n__________________________________________________________________________________________________\nactivation_92 (Activation)      (None, 28, 28, 128)  0           stage3_2_add[0][0]               \n__________________________________________________________________________________________________\nstage3_3_conv1 (Conv2D)         (None, 28, 28, 128)  147584      activation_92[0][0]              \n__________________________________________________________________________________________________\nstage3_3_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_3_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_93 (Activation)      (None, 28, 28, 128)  0           stage3_3_bn1[0][0]               \n__________________________________________________________________________________________________\nstage3_3_conv2 (Conv2D)         (None, 28, 28, 128)  147584      activation_93[0][0]              \n__________________________________________________________________________________________________\nstage3_3_bn2 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_3_conv2[0][0]             \n__________________________________________________________________________________________________\nstage3_3_add (Add)              (None, 28, 28, 128)  0           stage3_3_bn2[0][0]               \n                                                                 activation_92[0][0]              \n__________________________________________________________________________________________________\nactivation_94 (Activation)      (None, 28, 28, 128)  0           stage3_3_add[0][0]               \n__________________________________________________________________________________________________\nstage3_4_conv1 (Conv2D)         (None, 28, 28, 128)  147584      activation_94[0][0]              \n__________________________________________________________________________________________________\nstage3_4_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_4_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_95 (Activation)      (None, 28, 28, 128)  0           stage3_4_bn1[0][0]               \n__________________________________________________________________________________________________\nstage3_4_conv2 (Conv2D)         (None, 28, 28, 128)  147584      activation_95[0][0]              \n__________________________________________________________________________________________________\nstage3_4_bn2 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_4_conv2[0][0]             \n__________________________________________________________________________________________________\nstage3_4_add (Add)              (None, 28, 28, 128)  0           stage3_4_bn2[0][0]               \n                                                                 activation_94[0][0]              \n__________________________________________________________________________________________________\nactivation_96 (Activation)      (None, 28, 28, 128)  0           stage3_4_add[0][0]               \n__________________________________________________________________________________________________\nstage4_1_conv1 (Conv2D)         (None, 14, 14, 256)  295168      activation_96[0][0]              \n__________________________________________________________________________________________________\nstage4_1_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_1_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_97 (Activation)      (None, 14, 14, 256)  0           stage4_1_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_1_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_97[0][0]              \n__________________________________________________________________________________________________\nstage4_1_short (Conv2D)         (None, 14, 14, 256)  33024       activation_96[0][0]              \n__________________________________________________________________________________________________\nstage4_1_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_1_conv2[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_33 (BatchNo (None, 14, 14, 256)  1024        stage4_1_short[0][0]             \n__________________________________________________________________________________________________\nstage4_1_add (Add)              (None, 14, 14, 256)  0           stage4_1_bn2[0][0]               \n                                                                 batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nactivation_98 (Activation)      (None, 14, 14, 256)  0           stage4_1_add[0][0]               \n__________________________________________________________________________________________________\nstage4_2_conv1 (Conv2D)         (None, 14, 14, 256)  590080      activation_98[0][0]              \n__________________________________________________________________________________________________\nstage4_2_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_2_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_99 (Activation)      (None, 14, 14, 256)  0           stage4_2_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_2_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_99[0][0]              \n__________________________________________________________________________________________________\nstage4_2_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_2_conv2[0][0]             \n__________________________________________________________________________________________________\nstage4_2_add (Add)              (None, 14, 14, 256)  0           stage4_2_bn2[0][0]               \n                                                                 activation_98[0][0]              \n__________________________________________________________________________________________________\nactivation_100 (Activation)     (None, 14, 14, 256)  0           stage4_2_add[0][0]               \n__________________________________________________________________________________________________\nstage4_3_conv1 (Conv2D)         (None, 14, 14, 256)  590080      activation_100[0][0]             \n__________________________________________________________________________________________________\nstage4_3_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_3_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_101 (Activation)     (None, 14, 14, 256)  0           stage4_3_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_3_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_101[0][0]             \n__________________________________________________________________________________________________\nstage4_3_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_3_conv2[0][0]             \n__________________________________________________________________________________________________\nstage4_3_add (Add)              (None, 14, 14, 256)  0           stage4_3_bn2[0][0]               \n                                                                 activation_100[0][0]             \n__________________________________________________________________________________________________\nactivation_102 (Activation)     (None, 14, 14, 256)  0           stage4_3_add[0][0]               \n__________________________________________________________________________________________________\nstage4_4_conv1 (Conv2D)         (None, 14, 14, 256)  590080      activation_102[0][0]             \n__________________________________________________________________________________________________\nstage4_4_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_4_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_103 (Activation)     (None, 14, 14, 256)  0           stage4_4_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_4_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_103[0][0]             \n__________________________________________________________________________________________________\nstage4_4_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_4_conv2[0][0]             \n__________________________________________________________________________________________________\nstage4_4_add (Add)              (None, 14, 14, 256)  0           stage4_4_bn2[0][0]               \n                                                                 activation_102[0][0]             \n__________________________________________________________________________________________________\nactivation_104 (Activation)     (None, 14, 14, 256)  0           stage4_4_add[0][0]               \n__________________________________________________________________________________________________\nstage4_5_conv1 (Conv2D)         (None, 14, 14, 256)  590080      activation_104[0][0]             \n__________________________________________________________________________________________________\nstage4_5_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_5_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_105 (Activation)     (None, 14, 14, 256)  0           stage4_5_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_5_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_105[0][0]             \n__________________________________________________________________________________________________\nstage4_5_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_5_conv2[0][0]             \n__________________________________________________________________________________________________\nstage4_5_add (Add)              (None, 14, 14, 256)  0           stage4_5_bn2[0][0]               \n                                                                 activation_104[0][0]             \n__________________________________________________________________________________________________\nactivation_106 (Activation)     (None, 14, 14, 256)  0           stage4_5_add[0][0]               \n__________________________________________________________________________________________________\nstage4_6_conv1 (Conv2D)         (None, 14, 14, 256)  590080      activation_106[0][0]             \n__________________________________________________________________________________________________\nstage4_6_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_6_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_107 (Activation)     (None, 14, 14, 256)  0           stage4_6_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_6_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_107[0][0]             \n__________________________________________________________________________________________________\nstage4_6_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_6_conv2[0][0]             \n__________________________________________________________________________________________________\nstage4_6_add (Add)              (None, 14, 14, 256)  0           stage4_6_bn2[0][0]               \n                                                                 activation_106[0][0]             \n__________________________________________________________________________________________________\nactivation_108 (Activation)     (None, 14, 14, 256)  0           stage4_6_add[0][0]               \n__________________________________________________________________________________________________\nstage5_1_conv1 (Conv2D)         (None, 7, 7, 512)    1180160     activation_108[0][0]             \n__________________________________________________________________________________________________\nstage5_1_bn1 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_1_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_109 (Activation)     (None, 7, 7, 512)    0           stage5_1_bn1[0][0]               \n__________________________________________________________________________________________________\nstage5_1_conv2 (Conv2D)         (None, 7, 7, 512)    2359808     activation_109[0][0]             \n__________________________________________________________________________________________________\nstage5_1_short (Conv2D)         (None, 7, 7, 512)    131584      activation_108[0][0]             \n__________________________________________________________________________________________________\nstage5_1_bn2 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_1_conv2[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_39 (BatchNo (None, 7, 7, 512)    2048        stage5_1_short[0][0]             \n__________________________________________________________________________________________________\nstage5_1_add (Add)              (None, 7, 7, 512)    0           stage5_1_bn2[0][0]               \n                                                                 batch_normalization_39[0][0]     \n__________________________________________________________________________________________________\nactivation_110 (Activation)     (None, 7, 7, 512)    0           stage5_1_add[0][0]               \n__________________________________________________________________________________________________\nstage5_2_conv1 (Conv2D)         (None, 7, 7, 512)    2359808     activation_110[0][0]             \n__________________________________________________________________________________________________\nstage5_2_bn1 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_2_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_111 (Activation)     (None, 7, 7, 512)    0           stage5_2_bn1[0][0]               \n__________________________________________________________________________________________________\nstage5_2_conv2 (Conv2D)         (None, 7, 7, 512)    2359808     activation_111[0][0]             \n__________________________________________________________________________________________________\nstage5_2_bn2 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_2_conv2[0][0]             \n__________________________________________________________________________________________________\nstage5_2_add (Add)              (None, 7, 7, 512)    0           stage5_2_bn2[0][0]               \n                                                                 activation_110[0][0]             \n__________________________________________________________________________________________________\nactivation_112 (Activation)     (None, 7, 7, 512)    0           stage5_2_add[0][0]               \n__________________________________________________________________________________________________\nstage5_3_conv1 (Conv2D)         (None, 7, 7, 512)    2359808     activation_112[0][0]             \n__________________________________________________________________________________________________\nstage5_3_bn1 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_3_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_113 (Activation)     (None, 7, 7, 512)    0           stage5_3_bn1[0][0]               \n__________________________________________________________________________________________________\nstage5_3_conv2 (Conv2D)         (None, 7, 7, 512)    2359808     activation_113[0][0]             \n__________________________________________________________________________________________________\nstage5_3_bn2 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_3_conv2[0][0]             \n__________________________________________________________________________________________________\nstage5_3_add (Add)              (None, 7, 7, 512)    0           stage5_3_bn2[0][0]               \n                                                                 activation_112[0][0]             \n__________________________________________________________________________________________________\nactivation_114 (Activation)     (None, 7, 7, 512)    0           stage5_3_add[0][0]               \n__________________________________________________________________________________________________\navg_pool (AveragePooling2D)     (None, 4, 4, 512)    0           activation_114[0][0]             \n__________________________________________________________________________________________________\nflatten_11 (Flatten)            (None, 8192)         0           avg_pool[0][0]                   \n__________________________________________________________________________________________________\nfc2 (Dense)                     (None, 2)            16386       flatten_11[0][0]                 \n==================================================================================================\nTotal params: 21,326,594\nTrainable params: 21,309,570\nNon-trainable params: 17,024\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet34_input_layer = keras.layers.Input(shape=(224,224,3), name='input_layer')  \n",
    "resnet34_block_output = build_resnet(resnet34_input_layer)\n",
    "\n",
    "\n",
    "resnet34 = keras.Model(inputs=resnet34_input_layer, outputs=resnet34_block_output)\n",
    "resnet34.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1163/1163 [==============================] - 143s 105ms/step - loss: 0.7820 - accuracy: 0.5111 - val_loss: 0.6932 - val_accuracy: 0.5306\n",
      "Epoch 2/20\n",
      "1163/1163 [==============================] - 100s 86ms/step - loss: 1.2310 - accuracy: 0.5450 - val_loss: 2.3061 - val_accuracy: 0.6011\n",
      "Epoch 3/20\n",
      "1163/1163 [==============================] - 104s 89ms/step - loss: 2.5957 - accuracy: 0.5932 - val_loss: 3.1123 - val_accuracy: 0.6130\n",
      "Epoch 4/20\n",
      "1163/1163 [==============================] - 108s 93ms/step - loss: 3.4664 - accuracy: 0.6045 - val_loss: 1.0049 - val_accuracy: 0.5328\n",
      "Epoch 5/20\n",
      "1163/1163 [==============================] - 108s 93ms/step - loss: 2.6019 - accuracy: 0.6227 - val_loss: 2.0505 - val_accuracy: 0.6338\n",
      "Epoch 6/20\n",
      "1163/1163 [==============================] - 104s 89ms/step - loss: 3.1077 - accuracy: 0.6243 - val_loss: 5.1045 - val_accuracy: 0.5895\n",
      "Epoch 7/20\n",
      "1163/1163 [==============================] - 104s 89ms/step - loss: 3.9322 - accuracy: 0.6112 - val_loss: 4.9859 - val_accuracy: 0.6055\n",
      "Epoch 8/20\n",
      "1163/1163 [==============================] - 102s 88ms/step - loss: 3.1650 - accuracy: 0.6335 - val_loss: 3.2380 - val_accuracy: 0.6355\n",
      "Epoch 9/20\n",
      "1163/1163 [==============================] - 104s 89ms/step - loss: 3.2796 - accuracy: 0.6413 - val_loss: 1.9755 - val_accuracy: 0.6165\n",
      "Epoch 10/20\n",
      "1163/1163 [==============================] - 102s 88ms/step - loss: 2.6682 - accuracy: 0.6617 - val_loss: 1.6243 - val_accuracy: 0.6652\n",
      "Epoch 11/20\n",
      "1163/1163 [==============================] - 103s 88ms/step - loss: 2.5794 - accuracy: 0.6662 - val_loss: 2.1760 - val_accuracy: 0.6629\n",
      "Epoch 12/20\n",
      "1163/1163 [==============================] - 102s 88ms/step - loss: 2.9715 - accuracy: 0.6698 - val_loss: 1.3151 - val_accuracy: 0.6580\n",
      "Epoch 13/20\n",
      "1163/1163 [==============================] - 103s 89ms/step - loss: 2.4059 - accuracy: 0.6792 - val_loss: 1.9802 - val_accuracy: 0.6789\n",
      "Epoch 14/20\n",
      "1163/1163 [==============================] - 102s 88ms/step - loss: 2.2547 - accuracy: 0.7003 - val_loss: 1.9189 - val_accuracy: 0.6869\n",
      "Epoch 15/20\n",
      "1163/1163 [==============================] - 101s 87ms/step - loss: 2.3327 - accuracy: 0.6881 - val_loss: 3.9425 - val_accuracy: 0.6514\n",
      "Epoch 16/20\n",
      "1163/1163 [==============================] - 98s 85ms/step - loss: 2.1664 - accuracy: 0.6853 - val_loss: 1.8312 - val_accuracy: 0.6954\n",
      "Epoch 17/20\n",
      "1163/1163 [==============================] - 98s 85ms/step - loss: 1.7942 - accuracy: 0.7049 - val_loss: 3.6524 - val_accuracy: 0.6702\n",
      "Epoch 18/20\n",
      "1163/1163 [==============================] - 102s 88ms/step - loss: 2.5849 - accuracy: 0.6949 - val_loss: 2.4983 - val_accuracy: 0.7008\n",
      "Epoch 19/20\n",
      "1163/1163 [==============================] - 100s 86ms/step - loss: 2.3401 - accuracy: 0.7119 - val_loss: 2.1291 - val_accuracy: 0.7112\n",
      "Epoch 20/20\n",
      "1163/1163 [==============================] - 101s 87ms/step - loss: 2.1890 - accuracy: 0.7129 - val_loss: 1.7336 - val_accuracy: 0.7118\n"
     ]
    }
   ],
   "source": [
    "# ResNet-34 학습\n",
    "resnet34.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_34 = resnet34.fit(\n",
    "    ds_train,\n",
    "    steps_per_epoch=int(ds_info_train.splits['train[:80%]'].num_examples/BATCH_SIZE),\n",
    "    validation_steps=int(ds_info_valid.splits['train[80%:]'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_valid,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]               \n__________________________________________________________________________________________________\nstage2_3_conv3 (Conv2D)         (None, 56, 56, 256)  16640       activation_123[0][0]             \n__________________________________________________________________________________________________\nstage2_3_bn3 (BatchNormalizatio (None, 56, 56, 256)  1024        stage2_3_conv3[0][0]             \n__________________________________________________________________________________________________\nstage2_3_add (Add)              (None, 56, 56, 256)  0           stage2_3_bn3[0][0]               \n                                                                 activation_121[0][0]             \n__________________________________________________________________________________________________\nactivation_124 (Activation)     (None, 56, 56, 256)  0           stage2_3_add[0][0]               \n__________________________________________________________________________________________________\nstage3_1_conv1 (Conv2D)         (None, 28, 28, 128)  32896       activation_124[0][0]             \n__________________________________________________________________________________________________\nstage3_1_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_1_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_125 (Activation)     (None, 28, 28, 128)  0           stage3_1_bn1[0][0]               \n__________________________________________________________________________________________________\nstage3_1_conv2 (Conv2D)         (None, 28, 28, 128)  147584      activation_125[0][0]             \n__________________________________________________________________________________________________\nstage3_1_bn2 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_1_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_126 (Activation)     (None, 28, 28, 128)  0           stage3_1_bn2[0][0]               \n__________________________________________________________________________________________________\nstage3_1_conv3 (Conv2D)         (None, 28, 28, 512)  66048       activation_126[0][0]             \n__________________________________________________________________________________________________\nstage3_1_short (Conv2D)         (None, 28, 28, 512)  131584      activation_124[0][0]             \n__________________________________________________________________________________________________\nstage3_1_bn3 (BatchNormalizatio (None, 28, 28, 512)  2048        stage3_1_conv3[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_43 (BatchNo (None, 28, 28, 512)  2048        stage3_1_short[0][0]             \n__________________________________________________________________________________________________\nstage3_1_add (Add)              (None, 28, 28, 512)  0           stage3_1_bn3[0][0]               \n                                                                 batch_normalization_43[0][0]     \n__________________________________________________________________________________________________\nactivation_127 (Activation)     (None, 28, 28, 512)  0           stage3_1_add[0][0]               \n__________________________________________________________________________________________________\nstage3_2_conv1 (Conv2D)         (None, 28, 28, 128)  65664       activation_127[0][0]             \n__________________________________________________________________________________________________\nstage3_2_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_2_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_128 (Activation)     (None, 28, 28, 128)  0           stage3_2_bn1[0][0]               \n__________________________________________________________________________________________________\nstage3_2_conv2 (Conv2D)         (None, 28, 28, 128)  147584      activation_128[0][0]             \n__________________________________________________________________________________________________\nstage3_2_bn2 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_2_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_129 (Activation)     (None, 28, 28, 128)  0           stage3_2_bn2[0][0]               \n__________________________________________________________________________________________________\nstage3_2_conv3 (Conv2D)         (None, 28, 28, 512)  66048       activation_129[0][0]             \n__________________________________________________________________________________________________\nstage3_2_bn3 (BatchNormalizatio (None, 28, 28, 512)  2048        stage3_2_conv3[0][0]             \n__________________________________________________________________________________________________\nstage3_2_add (Add)              (None, 28, 28, 512)  0           stage3_2_bn3[0][0]               \n                                                                 activation_127[0][0]             \n__________________________________________________________________________________________________\nactivation_130 (Activation)     (None, 28, 28, 512)  0           stage3_2_add[0][0]               \n__________________________________________________________________________________________________\nstage3_3_conv1 (Conv2D)         (None, 28, 28, 128)  65664       activation_130[0][0]             \n__________________________________________________________________________________________________\nstage3_3_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_3_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_131 (Activation)     (None, 28, 28, 128)  0           stage3_3_bn1[0][0]               \n__________________________________________________________________________________________________\nstage3_3_conv2 (Conv2D)         (None, 28, 28, 128)  147584      activation_131[0][0]             \n__________________________________________________________________________________________________\nstage3_3_bn2 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_3_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_132 (Activation)     (None, 28, 28, 128)  0           stage3_3_bn2[0][0]               \n__________________________________________________________________________________________________\nstage3_3_conv3 (Conv2D)         (None, 28, 28, 512)  66048       activation_132[0][0]             \n__________________________________________________________________________________________________\nstage3_3_bn3 (BatchNormalizatio (None, 28, 28, 512)  2048        stage3_3_conv3[0][0]             \n__________________________________________________________________________________________________\nstage3_3_add (Add)              (None, 28, 28, 512)  0           stage3_3_bn3[0][0]               \n                                                                 activation_130[0][0]             \n__________________________________________________________________________________________________\nactivation_133 (Activation)     (None, 28, 28, 512)  0           stage3_3_add[0][0]               \n__________________________________________________________________________________________________\nstage3_4_conv1 (Conv2D)         (None, 28, 28, 128)  65664       activation_133[0][0]             \n__________________________________________________________________________________________________\nstage3_4_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_4_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_134 (Activation)     (None, 28, 28, 128)  0           stage3_4_bn1[0][0]               \n__________________________________________________________________________________________________\nstage3_4_conv2 (Conv2D)         (None, 28, 28, 128)  147584      activation_134[0][0]             \n__________________________________________________________________________________________________\nstage3_4_bn2 (BatchNormalizatio (None, 28, 28, 128)  512         stage3_4_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_135 (Activation)     (None, 28, 28, 128)  0           stage3_4_bn2[0][0]               \n__________________________________________________________________________________________________\nstage3_4_conv3 (Conv2D)         (None, 28, 28, 512)  66048       activation_135[0][0]             \n__________________________________________________________________________________________________\nstage3_4_bn3 (BatchNormalizatio (None, 28, 28, 512)  2048        stage3_4_conv3[0][0]             \n__________________________________________________________________________________________________\nstage3_4_add (Add)              (None, 28, 28, 512)  0           stage3_4_bn3[0][0]               \n                                                                 activation_133[0][0]             \n__________________________________________________________________________________________________\nactivation_136 (Activation)     (None, 28, 28, 512)  0           stage3_4_add[0][0]               \n__________________________________________________________________________________________________\nstage4_1_conv1 (Conv2D)         (None, 14, 14, 256)  131328      activation_136[0][0]             \n__________________________________________________________________________________________________\nstage4_1_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_1_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_137 (Activation)     (None, 14, 14, 256)  0           stage4_1_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_1_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_137[0][0]             \n__________________________________________________________________________________________________\nstage4_1_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_1_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_138 (Activation)     (None, 14, 14, 256)  0           stage4_1_bn2[0][0]               \n__________________________________________________________________________________________________\nstage4_1_conv3 (Conv2D)         (None, 14, 14, 1024) 263168      activation_138[0][0]             \n__________________________________________________________________________________________________\nstage4_1_short (Conv2D)         (None, 14, 14, 1024) 525312      activation_136[0][0]             \n__________________________________________________________________________________________________\nstage4_1_bn3 (BatchNormalizatio (None, 14, 14, 1024) 4096        stage4_1_conv3[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_47 (BatchNo (None, 14, 14, 1024) 4096        stage4_1_short[0][0]             \n__________________________________________________________________________________________________\nstage4_1_add (Add)              (None, 14, 14, 1024) 0           stage4_1_bn3[0][0]               \n                                                                 batch_normalization_47[0][0]     \n__________________________________________________________________________________________________\nactivation_139 (Activation)     (None, 14, 14, 1024) 0           stage4_1_add[0][0]               \n__________________________________________________________________________________________________\nstage4_2_conv1 (Conv2D)         (None, 14, 14, 256)  262400      activation_139[0][0]             \n__________________________________________________________________________________________________\nstage4_2_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_2_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_140 (Activation)     (None, 14, 14, 256)  0           stage4_2_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_2_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_140[0][0]             \n__________________________________________________________________________________________________\nstage4_2_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_2_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_141 (Activation)     (None, 14, 14, 256)  0           stage4_2_bn2[0][0]               \n__________________________________________________________________________________________________\nstage4_2_conv3 (Conv2D)         (None, 14, 14, 1024) 263168      activation_141[0][0]             \n__________________________________________________________________________________________________\nstage4_2_bn3 (BatchNormalizatio (None, 14, 14, 1024) 4096        stage4_2_conv3[0][0]             \n__________________________________________________________________________________________________\nstage4_2_add (Add)              (None, 14, 14, 1024) 0           stage4_2_bn3[0][0]               \n                                                                 activation_139[0][0]             \n__________________________________________________________________________________________________\nactivation_142 (Activation)     (None, 14, 14, 1024) 0           stage4_2_add[0][0]               \n__________________________________________________________________________________________________\nstage4_3_conv1 (Conv2D)         (None, 14, 14, 256)  262400      activation_142[0][0]             \n__________________________________________________________________________________________________\nstage4_3_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_3_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_143 (Activation)     (None, 14, 14, 256)  0           stage4_3_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_3_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_143[0][0]             \n__________________________________________________________________________________________________\nstage4_3_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_3_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_144 (Activation)     (None, 14, 14, 256)  0           stage4_3_bn2[0][0]               \n__________________________________________________________________________________________________\nstage4_3_conv3 (Conv2D)         (None, 14, 14, 1024) 263168      activation_144[0][0]             \n__________________________________________________________________________________________________\nstage4_3_bn3 (BatchNormalizatio (None, 14, 14, 1024) 4096        stage4_3_conv3[0][0]             \n__________________________________________________________________________________________________\nstage4_3_add (Add)              (None, 14, 14, 1024) 0           stage4_3_bn3[0][0]               \n                                                                 activation_142[0][0]             \n__________________________________________________________________________________________________\nactivation_145 (Activation)     (None, 14, 14, 1024) 0           stage4_3_add[0][0]               \n__________________________________________________________________________________________________\nstage4_4_conv1 (Conv2D)         (None, 14, 14, 256)  262400      activation_145[0][0]             \n__________________________________________________________________________________________________\nstage4_4_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_4_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_146 (Activation)     (None, 14, 14, 256)  0           stage4_4_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_4_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_146[0][0]             \n__________________________________________________________________________________________________\nstage4_4_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_4_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_147 (Activation)     (None, 14, 14, 256)  0           stage4_4_bn2[0][0]               \n__________________________________________________________________________________________________\nstage4_4_conv3 (Conv2D)         (None, 14, 14, 1024) 263168      activation_147[0][0]             \n__________________________________________________________________________________________________\nstage4_4_bn3 (BatchNormalizatio (None, 14, 14, 1024) 4096        stage4_4_conv3[0][0]             \n__________________________________________________________________________________________________\nstage4_4_add (Add)              (None, 14, 14, 1024) 0           stage4_4_bn3[0][0]               \n                                                                 activation_145[0][0]             \n__________________________________________________________________________________________________\nactivation_148 (Activation)     (None, 14, 14, 1024) 0           stage4_4_add[0][0]               \n__________________________________________________________________________________________________\nstage4_5_conv1 (Conv2D)         (None, 14, 14, 256)  262400      activation_148[0][0]             \n__________________________________________________________________________________________________\nstage4_5_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_5_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_149 (Activation)     (None, 14, 14, 256)  0           stage4_5_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_5_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_149[0][0]             \n__________________________________________________________________________________________________\nstage4_5_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_5_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_150 (Activation)     (None, 14, 14, 256)  0           stage4_5_bn2[0][0]               \n__________________________________________________________________________________________________\nstage4_5_conv3 (Conv2D)         (None, 14, 14, 1024) 263168      activation_150[0][0]             \n__________________________________________________________________________________________________\nstage4_5_bn3 (BatchNormalizatio (None, 14, 14, 1024) 4096        stage4_5_conv3[0][0]             \n__________________________________________________________________________________________________\nstage4_5_add (Add)              (None, 14, 14, 1024) 0           stage4_5_bn3[0][0]               \n                                                                 activation_148[0][0]             \n__________________________________________________________________________________________________\nactivation_151 (Activation)     (None, 14, 14, 1024) 0           stage4_5_add[0][0]               \n__________________________________________________________________________________________________\nstage4_6_conv1 (Conv2D)         (None, 14, 14, 256)  262400      activation_151[0][0]             \n__________________________________________________________________________________________________\nstage4_6_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_6_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_152 (Activation)     (None, 14, 14, 256)  0           stage4_6_bn1[0][0]               \n__________________________________________________________________________________________________\nstage4_6_conv2 (Conv2D)         (None, 14, 14, 256)  590080      activation_152[0][0]             \n__________________________________________________________________________________________________\nstage4_6_bn2 (BatchNormalizatio (None, 14, 14, 256)  1024        stage4_6_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_153 (Activation)     (None, 14, 14, 256)  0           stage4_6_bn2[0][0]               \n__________________________________________________________________________________________________\nstage4_6_conv3 (Conv2D)         (None, 14, 14, 1024) 263168      activation_153[0][0]             \n__________________________________________________________________________________________________\nstage4_6_bn3 (BatchNormalizatio (None, 14, 14, 1024) 4096        stage4_6_conv3[0][0]             \n__________________________________________________________________________________________________\nstage4_6_add (Add)              (None, 14, 14, 1024) 0           stage4_6_bn3[0][0]               \n                                                                 activation_151[0][0]             \n__________________________________________________________________________________________________\nactivation_154 (Activation)     (None, 14, 14, 1024) 0           stage4_6_add[0][0]               \n__________________________________________________________________________________________________\nstage5_1_conv1 (Conv2D)         (None, 7, 7, 512)    524800      activation_154[0][0]             \n__________________________________________________________________________________________________\nstage5_1_bn1 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_1_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_155 (Activation)     (None, 7, 7, 512)    0           stage5_1_bn1[0][0]               \n__________________________________________________________________________________________________\nstage5_1_conv2 (Conv2D)         (None, 7, 7, 512)    2359808     activation_155[0][0]             \n__________________________________________________________________________________________________\nstage5_1_bn2 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_1_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_156 (Activation)     (None, 7, 7, 512)    0           stage5_1_bn2[0][0]               \n__________________________________________________________________________________________________\nstage5_1_conv3 (Conv2D)         (None, 7, 7, 2048)   1050624     activation_156[0][0]             \n__________________________________________________________________________________________________\nstage5_1_short (Conv2D)         (None, 7, 7, 2048)   2099200     activation_154[0][0]             \n__________________________________________________________________________________________________\nstage5_1_bn3 (BatchNormalizatio (None, 7, 7, 2048)   8192        stage5_1_conv3[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_53 (BatchNo (None, 7, 7, 2048)   8192        stage5_1_short[0][0]             \n__________________________________________________________________________________________________\nstage5_1_add (Add)              (None, 7, 7, 2048)   0           stage5_1_bn3[0][0]               \n                                                                 batch_normalization_53[0][0]     \n__________________________________________________________________________________________________\nactivation_157 (Activation)     (None, 7, 7, 2048)   0           stage5_1_add[0][0]               \n__________________________________________________________________________________________________\nstage5_2_conv1 (Conv2D)         (None, 7, 7, 512)    1049088     activation_157[0][0]             \n__________________________________________________________________________________________________\nstage5_2_bn1 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_2_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_158 (Activation)     (None, 7, 7, 512)    0           stage5_2_bn1[0][0]               \n__________________________________________________________________________________________________\nstage5_2_conv2 (Conv2D)         (None, 7, 7, 512)    2359808     activation_158[0][0]             \n__________________________________________________________________________________________________\nstage5_2_bn2 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_2_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_159 (Activation)     (None, 7, 7, 512)    0           stage5_2_bn2[0][0]               \n__________________________________________________________________________________________________\nstage5_2_conv3 (Conv2D)         (None, 7, 7, 2048)   1050624     activation_159[0][0]             \n__________________________________________________________________________________________________\nstage5_2_bn3 (BatchNormalizatio (None, 7, 7, 2048)   8192        stage5_2_conv3[0][0]             \n__________________________________________________________________________________________________\nstage5_2_add (Add)              (None, 7, 7, 2048)   0           stage5_2_bn3[0][0]               \n                                                                 activation_157[0][0]             \n__________________________________________________________________________________________________\nactivation_160 (Activation)     (None, 7, 7, 2048)   0           stage5_2_add[0][0]               \n__________________________________________________________________________________________________\nstage5_3_conv1 (Conv2D)         (None, 7, 7, 512)    1049088     activation_160[0][0]             \n__________________________________________________________________________________________________\nstage5_3_bn1 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_3_conv1[0][0]             \n__________________________________________________________________________________________________\nactivation_161 (Activation)     (None, 7, 7, 512)    0           stage5_3_bn1[0][0]               \n__________________________________________________________________________________________________\nstage5_3_conv2 (Conv2D)         (None, 7, 7, 512)    2359808     activation_161[0][0]             \n__________________________________________________________________________________________________\nstage5_3_bn2 (BatchNormalizatio (None, 7, 7, 512)    2048        stage5_3_conv2[0][0]             \n__________________________________________________________________________________________________\nactivation_162 (Activation)     (None, 7, 7, 512)    0           stage5_3_bn2[0][0]               \n__________________________________________________________________________________________________\nstage5_3_conv3 (Conv2D)         (None, 7, 7, 2048)   1050624     activation_162[0][0]             \n__________________________________________________________________________________________________\nstage5_3_bn3 (BatchNormalizatio (None, 7, 7, 2048)   8192        stage5_3_conv3[0][0]             \n__________________________________________________________________________________________________\nstage5_3_add (Add)              (None, 7, 7, 2048)   0           stage5_3_bn3[0][0]               \n                                                                 activation_160[0][0]             \n__________________________________________________________________________________________________\nactivation_163 (Activation)     (None, 7, 7, 2048)   0           stage5_3_add[0][0]               \n__________________________________________________________________________________________________\navg_pool (AveragePooling2D)     (None, 4, 4, 2048)   0           activation_163[0][0]             \n__________________________________________________________________________________________________\nflatten_11 (Flatten)            (None, 32768)        0           avg_pool[0][0]                   \n__________________________________________________________________________________________________\nfc2 (Dense)                     (None, 2)            65538       flatten_11[0][0]                 \n==================================================================================================\nTotal params: 23,653,250\nTrainable params: 23,600,130\nNon-trainable params: 53,120\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#resnet50 모델 \n",
    "resnet50_input_layer = keras.layers.Input(shape=(224,224,3), name='input_layer')   # 입력 레이어 생성 = (224,224,3)\n",
    "resnet50_block_output = build_resnet(resnet50_input_layer, num_layer=50) # num_layer 설정\n",
    "\n",
    "resnet50 = keras.Model(inputs=resnet50_input_layer, outputs=resnet50_block_output)\n",
    "resnet50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1163/1163 [==============================] - 212s 148ms/step - loss: 0.7282 - accuracy: 0.5081 - val_loss: 0.6931 - val_accuracy: 0.4886\n",
      "Epoch 2/20\n",
      "1163/1163 [==============================] - 210s 181ms/step - loss: 0.6931 - accuracy: 0.4965 - val_loss: 0.6931 - val_accuracy: 0.4944\n",
      "Epoch 3/20\n",
      "1163/1163 [==============================] - 172s 148ms/step - loss: 0.6931 - accuracy: 0.5018 - val_loss: 0.6931 - val_accuracy: 0.4981\n",
      "Epoch 4/20\n",
      "1163/1163 [==============================] - 170s 146ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5002\n",
      "Epoch 5/20\n",
      "1163/1163 [==============================] - 176s 152ms/step - loss: 0.6931 - accuracy: 0.4991 - val_loss: 0.6931 - val_accuracy: 0.4970\n",
      "Epoch 6/20\n",
      "1163/1163 [==============================] - 177s 152ms/step - loss: 0.6931 - accuracy: 0.4960 - val_loss: 0.6931 - val_accuracy: 0.4836\n",
      "Epoch 7/20\n",
      "1163/1163 [==============================] - 179s 154ms/step - loss: 0.6931 - accuracy: 0.5056 - val_loss: 0.6931 - val_accuracy: 0.4970\n",
      "Epoch 8/20\n",
      "1163/1163 [==============================] - 177s 152ms/step - loss: 0.6931 - accuracy: 0.5025 - val_loss: 0.6931 - val_accuracy: 0.4942\n",
      "Epoch 9/20\n",
      "1163/1163 [==============================] - 170s 146ms/step - loss: 0.6931 - accuracy: 0.5029 - val_loss: 0.6931 - val_accuracy: 0.4953\n",
      "Epoch 10/20\n",
      "1163/1163 [==============================] - 171s 147ms/step - loss: 0.6931 - accuracy: 0.5029 - val_loss: 0.6931 - val_accuracy: 0.4981\n",
      "Epoch 11/20\n",
      "1163/1163 [==============================] - 175s 151ms/step - loss: 0.6931 - accuracy: 0.5030 - val_loss: 0.6931 - val_accuracy: 0.4918\n",
      "Epoch 12/20\n",
      "1163/1163 [==============================] - 177s 152ms/step - loss: 0.6931 - accuracy: 0.5059 - val_loss: 0.6931 - val_accuracy: 0.5006\n",
      "Epoch 13/20\n",
      "1163/1163 [==============================] - 181s 156ms/step - loss: 0.6931 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5022\n",
      "Epoch 14/20\n",
      "1163/1163 [==============================] - 187s 161ms/step - loss: 0.6931 - accuracy: 0.5023 - val_loss: 0.6931 - val_accuracy: 0.5013\n",
      "Epoch 15/20\n",
      "1163/1163 [==============================] - 192s 165ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.4944\n",
      "Epoch 16/20\n",
      "1163/1163 [==============================] - 182s 157ms/step - loss: 0.6931 - accuracy: 0.5051 - val_loss: 0.6931 - val_accuracy: 0.4959\n",
      "Epoch 17/20\n",
      "1163/1163 [==============================] - 188s 162ms/step - loss: 0.6931 - accuracy: 0.5017 - val_loss: 0.6994 - val_accuracy: 0.4953\n",
      "Epoch 18/20\n",
      "1163/1163 [==============================] - 188s 162ms/step - loss: 0.6931 - accuracy: 0.5047 - val_loss: 0.6931 - val_accuracy: 0.4888\n",
      "Epoch 19/20\n",
      "1163/1163 [==============================] - 189s 162ms/step - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6931 - val_accuracy: 0.4925\n",
      "Epoch 20/20\n",
      "1163/1163 [==============================] - 189s 163ms/step - loss: 0.6931 - accuracy: 0.4998 - val_loss: 0.6931 - val_accuracy: 0.4989\n"
     ]
    }
   ],
   "source": [
    "resnet50.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_50 = resnet50.fit(\n",
    "    ds_train,\n",
    "    steps_per_epoch=int(ds_info_train.splits['train[:80%]'].num_examples/BATCH_SIZE),\n",
    "    validation_steps=int(ds_info_valid.splits['train[80%:]'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_valid,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"276.833125pt\" version=\"1.1\" viewBox=\"0 0 384.828125 276.833125\" width=\"384.828125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-21T15:01:20.767721</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 276.833125 \nL 384.828125 276.833125 \nL 384.828125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 42.828125 239.664375 \nL 377.628125 239.664375 \nL 377.628125 22.224375 \nL 42.828125 22.224375 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m5905ba5d43\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.046307\" xlink:href=\"#m5905ba5d43\" y=\"239.664375\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(50.472088 254.184687)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 47.90625 32.703125 \nQ 47.90625 64.09375 30.703125 64.09375 \nQ 13.5 64.09375 13.5 32.703125 \nQ 13.5 1.203125 30.703125 1.203125 \nQ 47.90625 1.203125 47.90625 32.703125 \nz\nM 55.09375 32.703125 \nQ 55.09375 -5.09375 30.703125 -5.09375 \nQ 6.09375 -5.09375 6.09375 32.703125 \nQ 6.09375 48.40625 10.90625 58.203125 \nQ 17 70.5 30.703125 70.5 \nQ 44.203125 70.5 50.40625 58.203125 \nQ 52.703125 53.296875 53.890625 47 \nQ 55.09375 40.703125 55.09375 32.703125 \nz\n\" id=\"NanumGothic-48\"/>\n        <path d=\"M 10.796875 -4 \nQ 10.796875 -1.90625 12.25 -0.453125 \nQ 13.703125 1 15.796875 1 \nQ 17.90625 1 19.34375 -0.453125 \nQ 20.796875 -1.90625 20.796875 -4 \nQ 20.796875 -6.09375 19.34375 -7.546875 \nQ 17.90625 -9 15.796875 -9 \nQ 13.703125 -9 12.25 -7.546875 \nQ 10.796875 -6.09375 10.796875 -4 \nz\n\" id=\"NanumGothic-46\"/>\n       </defs>\n       <use xlink:href=\"#NanumGothic-48\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.094154\" xlink:href=\"#m5905ba5d43\" y=\"239.664375\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2.5 -->\n      <g transform=\"translate(90.519935 254.184687)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 9 -4.703125 \nQ 7.90625 -4.703125 7.34375 -4.640625 \nQ 6.796875 -4.59375 6.546875 -4.34375 \nQ 6.296875 -4.09375 6.25 -3.546875 \nQ 6.203125 -3 6.203125 -1.90625 \nL 6.296875 0.296875 \nQ 6.296875 1.59375 7.09375 2.40625 \nL 23.59375 20.59375 \nQ 42 40.5 42 50.796875 \nQ 42 56.796875 38 60.796875 \nQ 34.09375 64.703125 28.296875 64.703125 \nQ 20.203125 64.703125 11.40625 60.203125 \nQ 10 59.5 9.796875 60.796875 \nL 9.203125 64.09375 \nQ 8.90625 65.796875 10.203125 66.203125 \nQ 15.296875 68.5 19.390625 69.59375 \nQ 23.5 70.703125 27.796875 70.703125 \nQ 37.40625 70.703125 43.09375 65.90625 \nQ 49.5 60.703125 49.5 51.09375 \nQ 49.5 47.40625 47.703125 43 \nQ 45.90625 38.59375 42.703125 33.796875 \nQ 40.703125 31 37.046875 26.5 \nQ 33.40625 22 28.703125 16.90625 \nQ 25.203125 13.09375 21.75 9.1875 \nQ 18.296875 5.296875 14.796875 1.5 \nL 49.796875 1.5 \nQ 51.203125 1.5 51.203125 0.09375 \nL 51.203125 -3.296875 \nQ 51.203125 -4.703125 49.796875 -4.703125 \nz\n\" id=\"NanumGothic-50\"/>\n        <path d=\"M 11.5 -3.203125 \nQ 10.09375 -2.703125 10.09375 -1.40625 \nL 10.09375 2.796875 \nQ 10.09375 4.5 11.5 3.796875 \nQ 15.09375 2.09375 18.546875 1.390625 \nQ 22 0.703125 26.09375 0.703125 \nQ 34.90625 0.703125 40.40625 5 \nQ 46.40625 9.59375 46.40625 18.203125 \nQ 46.40625 35.09375 27.09375 35.09375 \nQ 23.296875 35.09375 20.25 34.6875 \nQ 17.203125 34.296875 13.5 33.296875 \nQ 12.40625 33 11.703125 33.546875 \nQ 11 34.09375 11 35.203125 \nL 11.59375 67.09375 \nQ 11.59375 69.203125 11.9375 69.546875 \nQ 12.296875 69.90625 14.40625 69.90625 \nL 48.59375 69.90625 \nQ 50 69.90625 50 68.5 \nL 50 65.203125 \nQ 50 63.796875 48.59375 63.796875 \nL 18.40625 63.796875 \nQ 18.296875 57.5 18.140625 51.5 \nQ 18 45.5 17.90625 39.203125 \nQ 22.796875 41.09375 29.796875 41.09375 \nQ 40.296875 41.09375 46.796875 35.40625 \nQ 53.296875 29.5 53.296875 19.59375 \nQ 53.296875 7.5 45.796875 0.90625 \nQ 38.59375 -5.40625 26.09375 -5.40625 \nQ 21.59375 -5.40625 18.296875 -4.90625 \nQ 15 -4.40625 11.5 -3.203125 \nz\n\" id=\"NanumGothic-53\"/>\n       </defs>\n       <use xlink:href=\"#NanumGothic-50\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"138.142001\" xlink:href=\"#m5905ba5d43\" y=\"239.664375\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5.0 -->\n      <g transform=\"translate(130.567782 254.184687)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-53\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.189847\" xlink:href=\"#m5905ba5d43\" y=\"239.664375\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 7.5 -->\n      <g transform=\"translate(170.615629 254.184687)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 24.203125 -3.296875 \nQ 23.5 -4.703125 22.203125 -4.703125 \nL 17.203125 -4.703125 \nQ 16.59375 -4.703125 16.34375 -4.296875 \nQ 16.09375 -3.90625 16.40625 -3.296875 \nL 45.5 63.796875 \nL 8.40625 63.796875 \nQ 7 63.796875 7 65.203125 \nL 7 68.5 \nQ 7 69.90625 8.40625 69.90625 \nL 49.90625 69.90625 \nQ 52 69.90625 52.34375 69.546875 \nQ 52.703125 69.203125 52.703125 67.09375 \nL 52.703125 64.59375 \nQ 52.703125 63.796875 52.40625 63.203125 \nz\n\" id=\"NanumGothic-55\"/>\n       </defs>\n       <use xlink:href=\"#NanumGothic-55\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"218.237694\" xlink:href=\"#m5905ba5d43\" y=\"239.664375\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10.0 -->\n      <g transform=\"translate(207.633788 254.184687)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 36.90625 -3.296875 \nQ 36.90625 -4.703125 35.5 -4.703125 \nL 31.40625 -4.703125 \nQ 30 -4.703125 30 -3.296875 \nL 30 61.703125 \nQ 26.90625 59.09375 24.203125 56.796875 \nQ 21.5 54.5 18.5 51.90625 \nQ 17.40625 51 16.5 52.09375 \nL 14.5 54.5 \nQ 14.203125 55 14.203125 55.546875 \nQ 14.203125 56.09375 14.703125 56.5 \nL 30.203125 69.5 \nQ 30.59375 69.90625 31.40625 69.90625 \nL 34.09375 69.90625 \nQ 36.203125 69.90625 36.546875 69.546875 \nQ 36.90625 69.203125 36.90625 67.09375 \nz\n\" id=\"NanumGothic-49\"/>\n       </defs>\n       <use xlink:href=\"#NanumGothic-49\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-48\"/>\n       <use x=\"121.199982\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"151.499969\" xlink:href=\"#NanumGothic-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"258.285541\" xlink:href=\"#m5905ba5d43\" y=\"239.664375\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12.5 -->\n      <g transform=\"translate(247.681635 254.184687)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-49\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-50\"/>\n       <use x=\"121.199982\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"151.499969\" xlink:href=\"#NanumGothic-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"298.333388\" xlink:href=\"#m5905ba5d43\" y=\"239.664375\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 15.0 -->\n      <g transform=\"translate(287.729482 254.184687)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-49\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-53\"/>\n       <use x=\"121.199982\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"151.499969\" xlink:href=\"#NanumGothic-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"338.381235\" xlink:href=\"#m5905ba5d43\" y=\"239.664375\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 17.5 -->\n      <g transform=\"translate(327.777329 254.184687)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-49\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-55\"/>\n       <use x=\"121.199982\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"151.499969\" xlink:href=\"#NanumGothic-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- Epoch -->\n     <g transform=\"translate(195.839063 267.67375)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 12.90625 -4.40625 \nQ 10.796875 -4.40625 10.4375 -4.046875 \nQ 10.09375 -3.703125 10.09375 -1.59375 \nL 10.09375 67.296875 \nQ 10.09375 69.203125 10.4375 69.546875 \nQ 10.796875 69.90625 12.703125 69.90625 \nL 47.40625 69.90625 \nQ 48.796875 69.90625 48.796875 68.5 \nL 48.796875 65 \nQ 48.796875 63.59375 47.40625 63.59375 \nL 17.40625 63.59375 \nL 17.40625 37.203125 \nL 45.90625 37.203125 \nQ 47.296875 37.203125 47.296875 35.796875 \nL 47.296875 32.203125 \nQ 47.296875 30.796875 45.90625 30.796875 \nL 17.40625 30.796875 \nL 17.40625 2 \nL 48.59375 2 \nQ 50 2 50 0.59375 \nL 50 -3 \nQ 50 -4.40625 48.59375 -4.40625 \nz\n\" id=\"NanumGothic-69\"/>\n       <path d=\"M 46.703125 23.5 \nQ 46.703125 27.59375 45.890625 31.4375 \nQ 45.09375 35.296875 43.1875 38.296875 \nQ 41.296875 41.296875 38.296875 43.140625 \nQ 35.296875 45 30.796875 45 \nQ 26.703125 45 23.703125 43.296875 \nQ 20.703125 41.59375 18.75 38.640625 \nQ 16.796875 35.703125 15.796875 31.796875 \nQ 14.796875 27.90625 14.796875 23.5 \nQ 14.796875 11.90625 19.140625 5.953125 \nQ 23.5 0 30.796875 0 \nQ 34.59375 0 37.546875 1.640625 \nQ 40.5 3.296875 42.546875 6.390625 \nQ 44.59375 9.5 45.640625 13.796875 \nQ 46.703125 18.09375 46.703125 23.5 \nz\nM 8.09375 48.40625 \nQ 8.09375 49.796875 9.5 49.796875 \nL 13.296875 49.796875 \nQ 14.703125 49.796875 14.703125 48.40625 \nL 14.203125 40.90625 \nQ 16.203125 45.296875 20.953125 48 \nQ 25.703125 50.703125 32 50.703125 \nQ 37.90625 50.703125 42.046875 48.59375 \nQ 46.203125 46.5 48.84375 42.84375 \nQ 51.5 39.203125 52.75 34.203125 \nQ 54 29.203125 54 23.5 \nQ 54 17.296875 52.546875 12 \nQ 51.09375 6.703125 48.34375 2.796875 \nQ 45.59375 -1.09375 41.5 -3.34375 \nQ 37.40625 -5.59375 32 -5.59375 \nQ 25.90625 -5.59375 21.5 -3.25 \nQ 17.09375 -0.90625 14.796875 3.296875 \nL 14.796875 -18.203125 \nQ 14.796875 -19.59375 13.40625 -19.59375 \nL 9.59375 -19.59375 \nQ 9 -19.59375 8.546875 -19.1875 \nQ 8.09375 -18.796875 8.09375 -18.203125 \nz\n\" id=\"NanumGothic-112\"/>\n       <path d=\"M 48.90625 22.703125 \nQ 48.90625 27.40625 47.65625 31.5 \nQ 46.40625 35.59375 44.203125 38.796875 \nQ 39 45.203125 30.703125 45.203125 \nQ 26.5 45.203125 23.140625 43.546875 \nQ 19.796875 41.90625 17.203125 38.796875 \nQ 14.90625 35.59375 13.65625 31.5 \nQ 12.40625 27.40625 12.40625 22.703125 \nQ 12.40625 17.90625 13.65625 13.796875 \nQ 14.90625 9.703125 17.203125 6.59375 \nQ 22.5 0.09375 30.703125 0.09375 \nQ 34.90625 0.09375 38.25 1.796875 \nQ 41.59375 3.5 44.203125 6.59375 \nQ 46.40625 9.703125 47.65625 13.796875 \nQ 48.90625 17.90625 48.90625 22.703125 \nz\nM 56.296875 22.703125 \nQ 56.296875 16.796875 54.59375 11.75 \nQ 52.90625 6.703125 50.09375 2.90625 \nQ 46.59375 -1.296875 41.75 -3.59375 \nQ 36.90625 -5.90625 30.703125 -5.90625 \nQ 24.5 -5.90625 19.59375 -3.59375 \nQ 14.703125 -1.296875 11.203125 2.90625 \nQ 8.40625 6.703125 6.75 11.75 \nQ 5.09375 16.796875 5.09375 22.703125 \nQ 5.09375 34.796875 11.203125 42.5 \nQ 18.203125 51.203125 30.703125 51.203125 \nQ 37 51.203125 41.796875 49.046875 \nQ 46.59375 46.90625 50.09375 42.5 \nQ 56.296875 34.59375 56.296875 22.703125 \nz\n\" id=\"NanumGothic-111\"/>\n       <path d=\"M 44 -4.703125 \nQ 41 -5.203125 38.203125 -5.390625 \nQ 35.40625 -5.59375 31.5 -5.59375 \nQ 25.296875 -5.59375 20.4375 -3.390625 \nQ 15.59375 -1.203125 12.25 2.59375 \nQ 8.90625 6.40625 7.09375 11.59375 \nQ 5.296875 16.796875 5.296875 22.796875 \nQ 5.296875 28.90625 7.1875 34.046875 \nQ 9.09375 39.203125 12.640625 42.953125 \nQ 16.203125 46.703125 21.140625 48.84375 \nQ 26.09375 51 32.296875 51 \nQ 35.59375 51 38.296875 50.75 \nQ 41 50.5 44.09375 49.796875 \nQ 45.5 49.40625 45.40625 47.90625 \nL 45.09375 44.703125 \nQ 44.90625 43.5 43.59375 43.796875 \nQ 41 44.59375 38.59375 44.9375 \nQ 36.203125 45.296875 33.296875 45.296875 \nQ 28.203125 45.296875 24.34375 43.640625 \nQ 20.5 42 17.890625 39 \nQ 15.296875 36 14 31.84375 \nQ 12.703125 27.703125 12.703125 22.796875 \nQ 12.703125 18.296875 14.046875 14.1875 \nQ 15.40625 10.09375 17.953125 6.9375 \nQ 20.5 3.796875 24.140625 1.9375 \nQ 27.796875 0.09375 32.5 0.09375 \nQ 36 0.09375 38.296875 0.390625 \nQ 40.59375 0.703125 43.5 1.5 \nQ 45 1.90625 45.09375 0.796875 \nL 45.40625 -2.90625 \nQ 45.5 -4.40625 44 -4.703125 \nz\n\" id=\"NanumGothic-99\"/>\n       <path d=\"M 52.09375 -2.90625 \nQ 52.09375 -4.296875 50.703125 -4.296875 \nL 46.703125 -4.296875 \nQ 45.296875 -4.296875 45.296875 -2.90625 \nL 45.296875 27.90625 \nQ 45.296875 32.703125 44.4375 36.046875 \nQ 43.59375 39.40625 42 41.453125 \nQ 40.40625 43.5 38.09375 44.390625 \nQ 35.796875 45.296875 33 45.296875 \nQ 24.796875 45.296875 20.4375 41.390625 \nQ 16.09375 37.5 16.09375 30.59375 \nL 16.09375 -2.90625 \nQ 16.09375 -4.296875 14.703125 -4.296875 \nL 10.703125 -4.296875 \nQ 9.296875 -4.296875 9.296875 -2.90625 \nL 9.296875 73.90625 \nQ 9.296875 75.296875 10.703125 75.296875 \nL 14.703125 75.296875 \nQ 16.09375 75.296875 16.09375 73.90625 \nL 16.09375 44.90625 \nQ 18.90625 48.09375 23.046875 49.546875 \nQ 27.203125 51 33.40625 51 \nQ 43.09375 51 47.59375 45.296875 \nQ 52.09375 39.59375 52.09375 28 \nz\n\" id=\"NanumGothic-104\"/>\n      </defs>\n      <use xlink:href=\"#NanumGothic-69\"/>\n      <use x=\"57.599991\" xlink:href=\"#NanumGothic-112\"/>\n      <use x=\"118.199982\" xlink:href=\"#NanumGothic-111\"/>\n      <use x=\"178.799973\" xlink:href=\"#NanumGothic-99\"/>\n      <use x=\"227.199966\" xlink:href=\"#NanumGothic-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mdd0ec48374\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mdd0ec48374\" y=\"208.99747\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.679688 212.757626)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-49\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mdd0ec48374\" y=\"175.131184\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.5 -->\n      <g transform=\"translate(20.679688 178.891341)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-49\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mdd0ec48374\" y=\"141.264898\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.0 -->\n      <g transform=\"translate(20.679688 145.025055)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-50\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mdd0ec48374\" y=\"107.398612\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2.5 -->\n      <g transform=\"translate(20.679688 111.158769)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-50\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mdd0ec48374\" y=\"73.532327\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 3.0 -->\n      <g transform=\"translate(20.679688 77.292483)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 7.703125 -3 \nQ 6.203125 -2.59375 6.40625 -0.703125 \nL 6.796875 3.203125 \nQ 7 4.40625 8.40625 3.796875 \nQ 12.59375 2.09375 16.34375 1.390625 \nQ 20.09375 0.703125 25.09375 0.59375 \nQ 33 0.5 38.40625 4.59375 \nQ 44.703125 8.796875 44.703125 16.90625 \nQ 44.703125 31.09375 23.40625 31.09375 \nL 18.203125 31.09375 \nQ 16.90625 31.09375 16.90625 32.203125 \nL 16.90625 35.90625 \nQ 16.90625 37.203125 18.203125 37.203125 \nL 18.703125 37.203125 \nQ 29.40625 37.203125 35.40625 39.5 \nQ 44.5 43 44.5 52.09375 \nQ 44.5 55 42.953125 57.453125 \nQ 41.40625 59.90625 38.796875 61.59375 \nQ 36.40625 63 33.40625 63.796875 \nQ 30.40625 64.59375 27.09375 64.59375 \nQ 23.703125 64.59375 19.59375 63.6875 \nQ 15.5 62.796875 11.703125 61.296875 \nQ 10.40625 60.796875 10.203125 62.09375 \nL 9.796875 65.796875 \nQ 9.5 67.203125 11.09375 67.796875 \nQ 15.59375 69.296875 19.5 70 \nQ 23.40625 70.703125 27.796875 70.703125 \nQ 37.90625 70.703125 44.296875 66.40625 \nQ 51.59375 61.5 51.59375 52.09375 \nQ 51.59375 45.40625 46.796875 40.5 \nQ 42.796875 36.203125 36.40625 34.203125 \nQ 39.59375 33.796875 42.34375 32.5 \nQ 45.09375 31.203125 47.09375 28.90625 \nQ 49.40625 26.40625 50.59375 23.25 \nQ 51.796875 20.09375 51.796875 16.296875 \nQ 51.796875 11.296875 49.75 7.25 \nQ 47.703125 3.203125 43.90625 0.296875 \nQ 40.203125 -2.40625 35.390625 -3.953125 \nQ 30.59375 -5.5 24.703125 -5.40625 \nQ 19.40625 -5.296875 15.546875 -4.796875 \nQ 11.703125 -4.296875 7.703125 -3 \nz\n\" id=\"NanumGothic-51\"/>\n       </defs>\n       <use xlink:href=\"#NanumGothic-51\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#mdd0ec48374\" y=\"39.666041\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 3.5 -->\n      <g transform=\"translate(20.679688 43.426197)scale(0.1 -0.1)\">\n       <use xlink:href=\"#NanumGothic-51\"/>\n       <use x=\"60.599991\" xlink:href=\"#NanumGothic-46\"/>\n       <use x=\"90.899979\" xlink:href=\"#NanumGothic-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Loss -->\n     <g transform=\"translate(14.720313 140.635)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 45.5 2 \nQ 46.90625 2 46.90625 0.59375 \nL 46.90625 -2.90625 \nQ 46.90625 -4.296875 45.5 -4.296875 \nL 13.09375 -4.296875 \nQ 11.703125 -4.296875 10.953125 -4.25 \nQ 10.203125 -4.203125 9.84375 -3.84375 \nQ 9.5 -3.5 9.453125 -2.703125 \nQ 9.40625 -1.90625 9.40625 -0.5 \nL 9.40625 68.296875 \nQ 9.40625 69.703125 10.796875 69.703125 \nL 15.40625 69.703125 \nQ 16.796875 69.703125 16.796875 68.296875 \nL 16.796875 2 \nz\n\" id=\"NanumGothic-76\"/>\n       <path d=\"M 6.59375 2.296875 \nQ 12.703125 -0.203125 19.203125 -0.203125 \nQ 24.59375 -0.203125 28.09375 2.4375 \nQ 31.59375 5.09375 31.59375 9.59375 \nQ 31.59375 12.203125 29.640625 14.640625 \nQ 27.703125 17.09375 24.90625 18.703125 \nQ 21.796875 20.40625 18.546875 21.953125 \nQ 15.296875 23.5 12.296875 25.203125 \nQ 8.703125 27.203125 7 30.140625 \nQ 5.296875 33.09375 5.296875 36.296875 \nQ 5.296875 40 6.6875 42.796875 \nQ 8.09375 45.59375 10.5 47.5 \nQ 12.90625 49.40625 16.09375 50.296875 \nQ 19.296875 51.203125 22.90625 51.203125 \nQ 26.703125 51.203125 29.5 50.796875 \nQ 32.296875 50.40625 35.09375 49.59375 \nQ 35.703125 49.40625 36.140625 49 \nQ 36.59375 48.59375 36.5 47.90625 \nL 36.203125 44.90625 \nQ 36.09375 44.09375 35.59375 43.890625 \nQ 35.09375 43.703125 34.59375 43.90625 \nQ 32.09375 44.796875 29.59375 45.140625 \nQ 27.09375 45.5 23.59375 45.5 \nQ 12 45.5 12 36.296875 \nQ 12 34.296875 13.546875 32.5 \nQ 15.09375 30.703125 18.796875 28.90625 \nQ 22 27.296875 25.09375 25.84375 \nQ 28.203125 24.40625 31.796875 22.40625 \nQ 34.90625 20.703125 36.953125 17.203125 \nQ 39 13.703125 39 9.59375 \nQ 39 5.59375 37.390625 2.6875 \nQ 35.796875 -0.203125 33.09375 -2.09375 \nQ 30.40625 -4 26.90625 -4.953125 \nQ 23.40625 -5.90625 19.703125 -5.90625 \nQ 17.5 -5.90625 15.75 -5.796875 \nQ 14 -5.703125 12.453125 -5.5 \nQ 10.90625 -5.296875 9.296875 -5 \nQ 7.703125 -4.703125 5.90625 -4.203125 \nQ 4.5 -3.703125 4.59375 -2.40625 \nL 5 1.296875 \nQ 5.203125 2.90625 6.59375 2.296875 \nz\n\" id=\"NanumGothic-115\"/>\n      </defs>\n      <use xlink:href=\"#NanumGothic-76\"/>\n      <use x=\"48.399994\" xlink:href=\"#NanumGothic-111\"/>\n      <use x=\"108.999985\" xlink:href=\"#NanumGothic-115\"/>\n      <use x=\"151.399979\" xlink:href=\"#NanumGothic-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p9c1e2c2d15)\" d=\"M 58.046307 227.616909 \nL 74.065446 136.586861 \nL 90.084584 85.516538 \nL 106.103723 46.660674 \nL 122.122862 69.787329 \nL 138.142001 56.907608 \nL 154.161139 32.108011 \nL 170.180278 60.732331 \nL 186.199417 53.70645 \nL 202.218556 107.100168 \nL 218.237694 86.395625 \nL 234.256833 97.996886 \nL 250.275972 92.877133 \nL 266.295111 109.440487 \nL 282.314249 109.02167 \nL 298.333388 138.476647 \nL 314.352527 139.244469 \nL 330.371666 111.307455 \nL 346.390804 114.816665 \nL 362.409943 144.469269 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p9c1e2c2d15)\" d=\"M 58.046307 229.394688 \nL 74.065446 229.780739 \nL 90.084584 229.780739 \nL 106.103723 229.780739 \nL 122.122862 229.780739 \nL 138.142001 229.780739 \nL 154.161139 229.780739 \nL 170.180278 229.780739 \nL 186.199417 229.780739 \nL 202.218556 229.780739 \nL 218.237694 229.780739 \nL 234.256833 229.780739 \nL 250.275972 229.780739 \nL 266.295111 229.780739 \nL 282.314249 229.780739 \nL 298.333388 229.780739 \nL 314.352527 229.780739 \nL 330.371666 229.780739 \nL 346.390804 229.780739 \nL 362.409943 229.780739 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 42.828125 239.664375 \nL 42.828125 22.224375 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 377.628125 239.664375 \nL 377.628125 22.224375 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 42.828125 239.664375 \nL 377.628125 239.664375 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 42.828125 22.224375 \nL 377.628125 22.224375 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Model training loss -->\n    <g transform=\"translate(157.446875 16.224375)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 86.296875 -2.90625 \nQ 86.296875 -4.296875 84.90625 -4.296875 \nL 80.40625 -4.296875 \nQ 79 -4.296875 79 -2.90625 \nL 79 63.40625 \nQ 71.59375 45 65.703125 30.5 \nQ 63.203125 24.296875 60.796875 18.34375 \nQ 58.40625 12.40625 56.5 7.75 \nQ 54.59375 3.09375 53.390625 0.1875 \nQ 52.203125 -2.703125 52.203125 -2.796875 \nQ 51.703125 -3.796875 50.703125 -3.796875 \nL 46.203125 -3.796875 \nQ 45.5 -3.796875 45.203125 -3.546875 \nQ 44.90625 -3.296875 44.703125 -2.796875 \nL 17.09375 63.40625 \nL 17.09375 -2.90625 \nQ 17.09375 -4.296875 15.703125 -4.296875 \nL 11.203125 -4.296875 \nQ 9.796875 -4.296875 9.796875 -2.90625 \nL 9.796875 67.09375 \nQ 9.796875 69.203125 10.140625 69.546875 \nQ 10.5 69.90625 12.59375 69.90625 \nL 20.09375 69.90625 \nQ 20.90625 69.90625 21.25 69.703125 \nQ 21.59375 69.5 21.90625 68.703125 \nL 48.5 4 \nL 74.296875 68.796875 \nQ 74.703125 69.90625 75.90625 69.90625 \nL 83.5 69.90625 \nQ 85.59375 69.90625 85.9375 69.546875 \nQ 86.296875 69.203125 86.296875 67.09375 \nz\n\" id=\"NanumGothic-77\"/>\n      <path d=\"M 13.703125 22.796875 \nQ 13.703125 18.09375 14.453125 14.25 \nQ 15.203125 10.40625 16.796875 7.5 \nQ 19 3.90625 22.140625 2 \nQ 25.296875 0.09375 29.703125 0.09375 \nQ 33.703125 0.09375 36.75 2.1875 \nQ 39.796875 4.296875 41.84375 7.546875 \nQ 43.90625 10.796875 44.90625 14.84375 \nQ 45.90625 18.90625 45.90625 22.796875 \nQ 45.90625 27.59375 44.84375 31.6875 \nQ 43.796875 35.796875 41.75 38.796875 \nQ 39.703125 41.796875 36.703125 43.5 \nQ 33.703125 45.203125 29.703125 45.203125 \nQ 21.203125 45.203125 16.796875 37.90625 \nQ 15.203125 35 14.453125 31.140625 \nQ 13.703125 27.296875 13.703125 22.796875 \nz\nM 52.59375 -2.90625 \nQ 52.59375 -4.296875 51.203125 -4.296875 \nL 47.296875 -4.296875 \nQ 45.90625 -4.296875 45.90625 -2.90625 \nL 45.90625 4.59375 \nQ 40.5 -5.59375 28.5 -5.59375 \nQ 17.796875 -5.59375 11.796875 2.59375 \nQ 6.296875 10.203125 6.296875 22.796875 \nQ 6.296875 50.90625 28.5 50.90625 \nQ 34.5 50.90625 39.140625 48.5 \nQ 43.796875 46.09375 45.90625 42.296875 \nL 45.90625 73.796875 \nQ 45.90625 75.203125 47.296875 75.203125 \nL 51.203125 75.203125 \nQ 52.59375 75.203125 52.59375 73.796875 \nz\n\" id=\"NanumGothic-100\"/>\n      <path d=\"M 42.703125 26.5 \nQ 42.703125 30.59375 41.890625 33.796875 \nQ 41.09375 37 39.40625 39.59375 \nQ 35.703125 45.5 28.703125 45.5 \nQ 21.5 45.5 17 39.5 \nQ 15.203125 36.796875 14 33.296875 \nQ 12.796875 29.796875 12.703125 26.5 \nz\nM 12.703125 20.796875 \nQ 12.59375 15.796875 13.84375 11.9375 \nQ 15.09375 8.09375 17.5 5.5 \nQ 19.90625 2.90625 23.25 1.5 \nQ 26.59375 0.09375 30.703125 0.09375 \nQ 34.703125 0.09375 38.203125 0.84375 \nQ 41.703125 1.59375 44.703125 3.296875 \nQ 46 3.90625 46 2.59375 \nL 46 -1.90625 \nQ 46 -2.796875 44.90625 -3.296875 \nQ 41.59375 -4.59375 38 -5.25 \nQ 34.40625 -5.90625 30.203125 -5.90625 \nQ 17.90625 -5.90625 11.59375 1.25 \nQ 5.296875 8.40625 5.296875 22.796875 \nQ 5.296875 35.5 11.59375 43.296875 \nQ 18.203125 51.203125 28.296875 51.203125 \nQ 33.90625 51.203125 38 49.203125 \nQ 42.09375 47.203125 45 43.203125 \nQ 50.09375 36.09375 50.09375 24.203125 \nQ 50.09375 22.90625 50.046875 22.203125 \nQ 50 21.5 49.703125 21.203125 \nQ 49.40625 20.90625 48.65625 20.84375 \nQ 47.90625 20.796875 46.5 20.796875 \nz\n\" id=\"NanumGothic-101\"/>\n      <path d=\"M 16.5 -2.90625 \nQ 16.5 -4.296875 15.09375 -4.296875 \nL 10.703125 -4.296875 \nQ 9.296875 -4.296875 9.296875 -2.90625 \nL 9.296875 73.796875 \nQ 9.296875 75.203125 10.703125 75.203125 \nL 15.09375 75.203125 \nQ 16.5 75.203125 16.5 73.796875 \nL 16.5 -2.90625 \nz\n\" id=\"NanumGothic-108\"/>\n      <path id=\"NanumGothic-32\"/>\n      <path d=\"M 34.5 -4.40625 \nQ 30.296875 -5.90625 26.203125 -5.90625 \nQ 18.09375 -5.90625 15.5 -0.59375 \nQ 13.796875 3 13.796875 12.09375 \nL 13.796875 44 \nL 4.40625 44 \nQ 3 44 3 45.40625 \nL 3 48.40625 \nQ 3 49.796875 4.40625 49.796875 \nL 13.796875 49.796875 \nL 13.796875 63.296875 \nQ 13.796875 64.796875 15.09375 64.796875 \nL 19.296875 64.796875 \nQ 20.59375 64.796875 20.59375 63.296875 \nL 20.59375 49.796875 \nL 31.703125 49.796875 \nQ 33.09375 49.796875 33.09375 48.40625 \nL 33.09375 45.40625 \nQ 33.09375 44 31.703125 44 \nL 20.59375 44 \nL 20.59375 8.703125 \nQ 20.59375 -0.203125 27.796875 -0.203125 \nQ 30.59375 -0.203125 34.09375 1 \nQ 34.59375 1.203125 35.046875 1 \nQ 35.5 0.796875 35.5 0.203125 \nL 35.703125 -2.40625 \nQ 35.90625 -3.796875 34.5 -4.40625 \nz\n\" id=\"NanumGothic-116\"/>\n      <path d=\"M 34.703125 43.703125 \nQ 32.90625 44.296875 29.90625 44.296875 \nQ 22.5 44.296875 18.90625 37 \nQ 16.09375 31.296875 16.09375 22.40625 \nL 16.09375 -2.90625 \nQ 16.09375 -4.296875 14.703125 -4.296875 \nL 10.796875 -4.296875 \nQ 9.40625 -4.296875 9.40625 -2.90625 \nL 9.40625 37.703125 \nQ 9.40625 41.296875 9.34375 43.546875 \nQ 9.296875 45.796875 9.203125 48.40625 \nQ 9.203125 49.796875 10.59375 49.796875 \nL 14.296875 49.796875 \nQ 15.703125 49.796875 15.703125 48.40625 \nL 15.703125 38.59375 \nQ 20.59375 51.203125 30.09375 51.203125 \nQ 32.40625 51.203125 34.703125 50.59375 \nQ 35.296875 50.40625 35.75 49.953125 \nQ 36.203125 49.5 36.203125 48.703125 \nL 36.203125 44.5 \nQ 36.203125 43.90625 35.703125 43.703125 \nQ 35.203125 43.5 34.703125 43.703125 \nz\n\" id=\"NanumGothic-114\"/>\n      <path d=\"M 40.09375 23.90625 \nL 38.203125 23.90625 \nQ 32.59375 24.09375 28.25 23.4375 \nQ 23.90625 22.796875 21 21.703125 \nQ 16.796875 20.203125 14.6875 17.25 \nQ 12.59375 14.296875 12.59375 10.203125 \nQ 12.59375 7.703125 13.390625 5.546875 \nQ 14.203125 3.40625 15.90625 2.203125 \nQ 18.90625 -0.203125 23.90625 -0.203125 \nQ 27.40625 -0.203125 30.34375 1.1875 \nQ 33.296875 2.59375 35.5 5.1875 \nQ 37.703125 7.796875 38.890625 11.640625 \nQ 40.09375 15.5 40.09375 20.5 \nz\nM 11.59375 41.40625 \nQ 11.09375 41.09375 10.640625 41.25 \nQ 10.203125 41.40625 10.203125 41.90625 \nL 10.203125 45.703125 \nQ 10.203125 47.09375 11.59375 47.796875 \nQ 15.09375 49.703125 19.046875 50.453125 \nQ 23 51.203125 27.703125 51.203125 \nQ 37.703125 51.203125 42.296875 46.453125 \nQ 46.90625 41.703125 46.90625 32.09375 \nL 46.90625 7 \nQ 46.90625 4 47 1.640625 \nQ 47.09375 -0.703125 47.296875 -2.90625 \nQ 47.296875 -4.296875 45.90625 -4.296875 \nL 42.296875 -4.296875 \nQ 40.90625 -4.296875 40.90625 -2.90625 \nL 40.90625 4.796875 \nQ 38.796875 -0.296875 33.84375 -3.09375 \nQ 28.90625 -5.90625 23.09375 -5.90625 \nQ 13 -5.90625 8.296875 0 \nQ 6.703125 2 5.890625 4.640625 \nQ 5.09375 7.296875 5.09375 9.90625 \nQ 5.09375 16 7.9375 20.390625 \nQ 10.796875 24.796875 16.703125 27 \nQ 23.90625 29.59375 38.796875 29.59375 \nL 40.09375 29.59375 \nL 40.09375 32.59375 \nQ 40.09375 38.90625 37.25 42.203125 \nQ 34.40625 45.5 27.703125 45.5 \nQ 23.203125 45.5 19.203125 44.546875 \nQ 15.203125 43.59375 11.59375 41.40625 \nz\n\" id=\"NanumGothic-97\"/>\n      <path d=\"M 16.40625 -2.90625 \nQ 16.40625 -4.296875 15 -4.296875 \nL 10.59375 -4.296875 \nQ 9.203125 -4.296875 9.203125 -2.90625 \nL 9.203125 48.40625 \nQ 9.203125 49.796875 10.59375 49.796875 \nL 15 49.796875 \nQ 16.40625 49.796875 16.40625 48.40625 \nL 16.40625 -2.90625 \nz\nM 7.90625 68.703125 \nQ 7.90625 70.703125 9.296875 72.140625 \nQ 10.703125 73.59375 12.703125 73.59375 \nQ 14.703125 73.59375 16.140625 72.140625 \nQ 17.59375 70.703125 17.59375 68.703125 \nQ 17.59375 66.703125 16.140625 65.296875 \nQ 14.703125 63.90625 12.703125 63.90625 \nQ 10.703125 63.90625 9.296875 65.296875 \nQ 7.90625 66.703125 7.90625 68.703125 \nz\n\" id=\"NanumGothic-105\"/>\n      <path d=\"M 52.09375 -2.90625 \nQ 52.09375 -4.296875 50.703125 -4.296875 \nL 46.703125 -4.296875 \nQ 45.296875 -4.296875 45.296875 -2.90625 \nL 45.296875 28.59375 \nQ 45.296875 37.796875 42 41.640625 \nQ 38.703125 45.5 32.703125 45.5 \nQ 24.59375 45.5 20.34375 41 \nQ 16.09375 36.5 16.09375 28.40625 \nL 16.09375 -2.90625 \nQ 16.09375 -4.296875 14.703125 -4.296875 \nL 10.703125 -4.296875 \nQ 9.296875 -4.296875 9.296875 -2.90625 \nL 9.296875 37.09375 \nQ 9.296875 41.40625 9.25 43.796875 \nQ 9.203125 46.203125 9 48.40625 \nQ 8.90625 49.09375 9.34375 49.4375 \nQ 9.796875 49.796875 10.40625 49.796875 \nL 14 49.796875 \nQ 15.40625 49.796875 15.40625 48.40625 \nL 15.40625 42.90625 \nQ 18.203125 47.203125 22.140625 49.203125 \nQ 26.09375 51.203125 33 51.203125 \nQ 42.5 51.203125 47.296875 45.703125 \nQ 52.09375 40.203125 52.09375 30.203125 \nz\n\" id=\"NanumGothic-110\"/>\n      <path d=\"M 46.09375 23 \nQ 46.09375 33.703125 41.6875 39.5 \nQ 37.296875 45.296875 29.796875 45.296875 \nQ 21.703125 45.296875 17.59375 39.890625 \nQ 13.5 34.5 13.5 22.796875 \nQ 13.5 18.5 14.703125 14.890625 \nQ 15.90625 11.296875 18 8.75 \nQ 20.09375 6.203125 23 4.75 \nQ 25.90625 3.296875 29.296875 3.296875 \nQ 32.796875 3.296875 35.890625 4.75 \nQ 39 6.203125 41.25 8.84375 \nQ 43.5 11.5 44.796875 15.09375 \nQ 46.09375 18.703125 46.09375 23 \nz\nM 11.703125 -17.40625 \nQ 10.5 -17.09375 10.703125 -15.90625 \nL 11 -11.5 \nQ 11 -10.203125 12.296875 -10.90625 \nQ 16.09375 -12.59375 19.59375 -13.34375 \nQ 23.09375 -14.09375 27.203125 -14.09375 \nQ 36.296875 -14.09375 40.84375 -8.546875 \nQ 45.40625 -3 46 6.59375 \nQ 43.703125 2.09375 39.203125 -0.203125 \nQ 34.703125 -2.5 29.203125 -2.5 \nQ 24.296875 -2.5 20.09375 -0.703125 \nQ 15.90625 1.09375 12.796875 4.390625 \nQ 9.703125 7.703125 7.890625 12.390625 \nQ 6.09375 17.09375 6.09375 22.796875 \nQ 6.09375 29.5 7.34375 34.75 \nQ 8.59375 40 11.296875 43.59375 \nQ 14 47.203125 18.296875 49.09375 \nQ 22.59375 51 28.703125 51 \nQ 34 51 38.046875 49.25 \nQ 42.09375 47.5 46.296875 43 \nL 46.09375 48.40625 \nQ 46.09375 49.796875 47.5 49.796875 \nL 51.296875 49.796875 \nQ 52.703125 49.796875 52.703125 48.40625 \nL 52.703125 8.40625 \nQ 52.703125 1.90625 51.046875 -3.34375 \nQ 49.40625 -8.59375 46.15625 -12.25 \nQ 42.90625 -15.90625 38.09375 -17.84375 \nQ 33.296875 -19.796875 27.09375 -19.796875 \nQ 22.59375 -19.796875 19.1875 -19.25 \nQ 15.796875 -18.703125 11.703125 -17.40625 \nz\n\" id=\"NanumGothic-103\"/>\n     </defs>\n     <use xlink:href=\"#NanumGothic-77\"/>\n     <use x=\"96.899994\" xlink:href=\"#NanumGothic-111\"/>\n     <use x=\"157.499985\" xlink:href=\"#NanumGothic-100\"/>\n     <use x=\"218.099976\" xlink:href=\"#NanumGothic-101\"/>\n     <use x=\"272.59996\" xlink:href=\"#NanumGothic-108\"/>\n     <use x=\"296.799957\" xlink:href=\"#NanumGothic-32\"/>\n     <use x=\"324.799942\" xlink:href=\"#NanumGothic-116\"/>\n     <use x=\"361.09993\" xlink:href=\"#NanumGothic-114\"/>\n     <use x=\"397.399918\" xlink:href=\"#NanumGothic-97\"/>\n     <use x=\"451.899902\" xlink:href=\"#NanumGothic-105\"/>\n     <use x=\"476.099899\" xlink:href=\"#NanumGothic-110\"/>\n     <use x=\"536.69989\" xlink:href=\"#NanumGothic-105\"/>\n     <use x=\"560.899887\" xlink:href=\"#NanumGothic-110\"/>\n     <use x=\"621.499878\" xlink:href=\"#NanumGothic-103\"/>\n     <use x=\"682.099869\" xlink:href=\"#NanumGothic-32\"/>\n     <use x=\"710.099854\" xlink:href=\"#NanumGothic-108\"/>\n     <use x=\"734.29985\" xlink:href=\"#NanumGothic-111\"/>\n     <use x=\"794.899841\" xlink:href=\"#NanumGothic-115\"/>\n     <use x=\"837.299835\" xlink:href=\"#NanumGothic-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 49.828125 59.18375 \nL 122.40625 59.18375 \nQ 124.40625 59.18375 124.40625 57.18375 \nL 124.40625 29.224375 \nQ 124.40625 27.224375 122.40625 27.224375 \nL 49.828125 27.224375 \nQ 47.828125 27.224375 47.828125 29.224375 \nL 47.828125 57.18375 \nQ 47.828125 59.18375 49.828125 59.18375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 51.828125 35.244687 \nL 71.828125 35.244687 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_18\">\n     <!-- resnet34 -->\n     <g transform=\"translate(79.828125 38.744687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 39.203125 19 \nL 39.203125 63.59375 \nQ 31.796875 52.40625 24.5 41.296875 \nQ 17.203125 30.203125 9.703125 19 \nz\nM 46 -3.296875 \nQ 46 -4.703125 44.59375 -4.703125 \nL 40.59375 -4.703125 \nQ 39.203125 -4.703125 39.203125 -3.296875 \nL 39.203125 12.90625 \nL 6.203125 12.90625 \nQ 4.09375 12.90625 3.75 13.25 \nQ 3.40625 13.59375 3.40625 15.703125 \nL 3.40625 18.796875 \nQ 3.40625 19.703125 3.5 20 \nQ 3.59375 20.296875 4.09375 21.09375 \nL 36 68.703125 \nQ 36.59375 69.59375 36.84375 69.75 \nQ 37.09375 69.90625 38.203125 69.90625 \nL 43.203125 69.90625 \nQ 45.296875 69.90625 45.640625 69.546875 \nQ 46 69.203125 46 67.09375 \nL 46 19 \nL 55.90625 19 \nQ 57.296875 19 57.296875 17.59375 \nL 57.296875 14.296875 \nQ 57.296875 12.90625 55.90625 12.90625 \nL 46 12.90625 \nz\n\" id=\"NanumGothic-52\"/>\n      </defs>\n      <use xlink:href=\"#NanumGothic-114\"/>\n      <use x=\"36.299988\" xlink:href=\"#NanumGothic-101\"/>\n      <use x=\"90.799973\" xlink:href=\"#NanumGothic-115\"/>\n      <use x=\"133.199966\" xlink:href=\"#NanumGothic-110\"/>\n      <use x=\"193.799957\" xlink:href=\"#NanumGothic-101\"/>\n      <use x=\"248.299942\" xlink:href=\"#NanumGothic-116\"/>\n      <use x=\"284.59993\" xlink:href=\"#NanumGothic-51\"/>\n      <use x=\"345.199921\" xlink:href=\"#NanumGothic-52\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 51.828125 49.724375 \nL 71.828125 49.724375 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_19\">\n     <!-- resnet50 -->\n     <g transform=\"translate(79.828125 53.224375)scale(0.1 -0.1)\">\n      <use xlink:href=\"#NanumGothic-114\"/>\n      <use x=\"36.299988\" xlink:href=\"#NanumGothic-101\"/>\n      <use x=\"90.799973\" xlink:href=\"#NanumGothic-115\"/>\n      <use x=\"133.199966\" xlink:href=\"#NanumGothic-110\"/>\n      <use x=\"193.799957\" xlink:href=\"#NanumGothic-101\"/>\n      <use x=\"248.299942\" xlink:href=\"#NanumGothic-116\"/>\n      <use x=\"284.59993\" xlink:href=\"#NanumGothic-53\"/>\n      <use x=\"345.199921\" xlink:href=\"#NanumGothic-48\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9c1e2c2d15\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"42.828125\" y=\"22.224375\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# resnet34 vs resnet 50 학습 loss 시각화\n",
    "plt.plot(history_34.history['loss'], 'r')\n",
    "plt.plot(history_50.history['loss'], 'b')\n",
    "plt.title('Model training loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['resnet34', 'resnet50'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 프로젝트를 마치며..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 모델을 실제로 구성해보고 하나하나 검색하며 알아보는 시간이였다 하지만 loss 들은 줄어들지 않네요... 어디서 문제 인지 잘모르겠다 나의 이해력이 부족인가... 알면 알수록 더 어려운 딥러닝 ResNet34,50을 구현을 해본거에 의미를 두기로 했다..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}