{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('aiffel': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0af283f273ba8452268ed83ca0fd9f2c86ae0020769dca83ca1aff968103fc19"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 프로젝트 : Pretrained model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Step 1. pretrained model 로딩 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Step 1-1. 데이터 불러오기 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json_tree(data, indent=\"\"):\n",
    "    for key, value in data.items():\n",
    "        if type(value) == list:     # list 형태의 item은 첫번째 item만 출력\n",
    "            print(f'{indent}- {key}: [{len(value)}]')\n",
    "            print_json_tree(value[0], indent + \"  \")\n",
    "        else:\n",
    "            print(f'{indent}- {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_qna/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_qna/models'\n",
    "\n",
    "# 훈련데이터 확인\n",
    "train_json_path = data_dir + '/KorQuAD_v1.0_train.json'\n",
    "with open(train_json_path) as f:\n",
    "    train_json = json.load(f)\n",
    "    print_json_tree(train_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증데이터 확인\n",
    "dev_json_path = data_dir + '/KorQuAD_v1.0_dev.json'\n",
    "with open(dev_json_path) as f:\n",
    "    dev_json = json.load(f)\n",
    "    print_json_tree(dev_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(train_json[\"data\"][0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "source": [
    "#### Step 1-2. 데이터셋 전처리 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 단위 정보 \n",
    "def _is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitespace가 2개인 경우를 처리해야 함\n",
    "\n",
    "string1 = '1839년 파우스트을 읽었다.'\n",
    "string2 = '1839년  파우스트을 읽었다.'\n",
    "string1[6:10], string2[7:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_whitespace(string):\n",
    "    word_tokens = []\n",
    "    char_to_word = []\n",
    "    prev_is_whitespace = True\n",
    "\n",
    "    for c in string:\n",
    "        if _is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "        else:\n",
    "            if prev_is_whitespace:\n",
    "                word_tokens.append(c)\n",
    "            else:\n",
    "                word_tokens[-1] += c\n",
    "            prev_is_whitespace = False    \n",
    "        char_to_word.append(len(word_tokens) - 1)\n",
    "    \n",
    "    return word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 문장(string1)에 대해 띄어쓰기 영역 정보를 표시\n",
    "word_tokens, char_to_word = _tokenize_whitespace(string1)\n",
    "for c, i in zip(list(string1), char_to_word):\n",
    "    print(f'\\'{c}\\' : {i}')\n",
    "\n",
    "word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 문장(string2)에 대해 띄어쓰기 영역 정보를 표시\n",
    "word_tokens, char_to_word = _tokenize_whitespace(string2)\n",
    "for c, i in zip(list(string2), char_to_word):\n",
    "    print(f'\\'{c}\\' : {i}')\n",
    "\n",
    "word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")\n",
    "\n",
    "# word를 subword로 변경하면서 index 저장\n",
    "word_to_token = []\n",
    "context_tokens = []\n",
    "for (i, word) in enumerate(word_tokens):\n",
    "    word_to_token.append(len(context_tokens))\n",
    "    tokens = vocab.encode_as_pieces(word)  # SentencePiece를 사용해 Subword로 쪼갭니다.\n",
    "    for token in tokens:\n",
    "        context_tokens.append(token)\n",
    "\n",
    "context_tokens, word_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_vocab(vocab, context_words):\n",
    "    word_to_token = []\n",
    "    context_tokens = []\n",
    "    for (i, word) in enumerate(context_words):\n",
    "        word_to_token.append(len(context_tokens))\n",
    "        tokens = vocab.encode_as_pieces(word)\n",
    "        for token in tokens:\n",
    "            context_tokens.append(token)\n",
    "    return context_tokens, word_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokens)  # 처리해야 할 word 단위 입력\n",
    "\n",
    "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
    "context_tokens, word_to_token   # Subword 단위로 토큰화한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve Span\n",
    "context = train_json['data'][0]['paragraphs'][0]['context']\n",
    "question = train_json['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "answer_text = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n",
    "answer_start = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "print('[context] ', context)\n",
    "print('[question] ', question)\n",
    "print('[answer] ', answer_text)\n",
    "print('[answer_start] index: ', answer_start, 'character: ', context[answer_start])\n",
    "print('[answer_end]index: ', answer_end, 'character: ', context[answer_end])\n",
    "\n",
    "# answer_text에 해당하는 context 영역을 정확히 찾아내야 합니다. \n",
    "assert context[answer_start:answer_end + 1] == answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context를 띄어쓰기(word) 단위로 토큰화한 결과를 살펴봅니다. \n",
    "word_tokens, char_to_word = _tokenize_whitespace(context)\n",
    "\n",
    "print( word_tokens[:20])\n",
    "\n",
    "char_to_word[:20], context[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기(word) 단위로 쪼개진 context(word_tokens)를 Subword로 토큰화한 결과를 살펴봅니다. \n",
    "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
    "for i in range(min(20, len(word_to_token) - 1)):\n",
    "    print(word_to_token[i], context_tokens[word_to_token[i]:word_to_token[i + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_start와 answer_end로부터 word_start와 word_end를 구합니다. \n",
    "word_start = char_to_word[answer_start]\n",
    "word_end = char_to_word[answer_end]\n",
    "word_start, word_end, answer_text, word_tokens[word_start:word_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start = word_to_token[word_start]\n",
    "if word_end < len(word_to_token) - 1:\n",
    "    token_end = word_to_token[word_end + 1] - 1\n",
    "else:\n",
    "    token_end = len(context_tokens) - 1\n",
    "token_start, token_end, context_tokens[token_start:token_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 정답인 answer_text도 Subword 기준으로 토큰화해 둡니다. \n",
    "token_answer = \" \".join(vocab.encode_as_pieces(answer_text))\n",
    "token_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답이 될수 있는 new_start와 new_end의 경우를 순회탐색합니다. \n",
    "for new_start in range(token_start, token_end + 1):\n",
    "    for new_end in range(token_end, new_start - 1, -1):\n",
    "        text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
    "        if text_span == token_answer:   # 정답과 일치하는 경우\n",
    "            print(\"O >>\", (new_start, new_end), text_span)\n",
    "        else:\n",
    "            print(\"X >>\", (new_start, new_end), text_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_tokens에서 char_answer의 위치를 찾아 리턴하는 함수\n",
    "def _improve_span(vocab, context_tokens, token_start, token_end, char_answer):\n",
    "    token_answer = \" \".join(vocab.encode_as_pieces(char_answer))\n",
    "    for new_start in range(token_start, token_end + 1):\n",
    "        for new_end in range(token_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
    "            if text_span == token_answer:\n",
    "                return (new_start, new_end)\n",
    "    return (token_start, token_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, answer_text)\n",
    "print('token_start:', token_start, ' token_end:', token_end)\n",
    "context_tokens[token_start:token_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분리\n",
    "def dump_korquad(vocab, json_data, out_file):\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for data in tqdm(json_data[\"data\"]):\n",
    "            title = data[\"title\"]\n",
    "            for paragraph in data[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]\n",
    "                context_words, char_to_word = _tokenize_whitespace(context)\n",
    "\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    assert len(qa[\"answers\"]) == 1\n",
    "                    qa_id = qa[\"id\"]\n",
    "                    question = qa[\"question\"]\n",
    "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                    answer_start = qa[\"answers\"][0][\"answer_start\"]\n",
    "                    answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "                    assert answer_text == context[answer_start:answer_end + 1]\n",
    "\n",
    "                    word_start = char_to_word[answer_start]\n",
    "                    word_end = char_to_word[answer_end]\n",
    "\n",
    "                    word_answer = \" \".join(context_words[word_start:word_end + 1])\n",
    "                    char_answer = \" \".join(answer_text.strip().split())\n",
    "                    assert char_answer in word_answer\n",
    "\n",
    "                    context_tokens, word_to_token = _tokenize_vocab(vocab, context_words)\n",
    "\n",
    "                    token_start = word_to_token[word_start]\n",
    "                    if word_end < len(word_to_token) - 1:\n",
    "                        token_end = word_to_token[word_end + 1] - 1\n",
    "                    else:\n",
    "                        token_end = len(context_tokens) - 1\n",
    "\n",
    "                    token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, char_answer)\n",
    "\n",
    "                    data = {\"qa_id\": qa_id, \"title\": title, \"question\": vocab.encode_as_pieces(question), \"context\": context_tokens, \"answer\": char_answer, \"token_start\": token_start, \"token_end\":token_end}\n",
    "                    f.write(json.dumps(data, ensure_ascii=False))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리를 수행하여 파일로 생성합니다. \n",
    "dump_korquad(vocab, train_json, f\"{data_dir}/korquad_train.json\")\n",
    "dump_korquad(vocab, dev_json, f\"{data_dir}/korquad_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_file(filename, count=10):\n",
    "    \"\"\"\n",
    "    파일 내용 출력\n",
    "    :param filename: 파일 이름\n",
    "    :param count: 출력 라인 수\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if count <= i:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "print_file(f\"{data_dir}/korquad_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석 \n",
    "questions = []\n",
    "contexts = []\n",
    "token_starts = []\n",
    "with open(f\"{data_dir}/korquad_train.json\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        questions.append(data[\"question\"])\n",
    "        contexts.append(data[\"context\"])\n",
    "        token_starts.append(data[\"token_start\"])\n",
    "        if i < 10:\n",
    "            print(data[\"token_start\"], data[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token count\n",
    "train_question_counts = [len(question) for question in questions]\n",
    "train_question_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_question_counts, bins=100, range=[0, 100], facecolor='b', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Count of question')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of question')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of question')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"question 길이 최대:    {np.max(train_question_counts):4d}\")\n",
    "print(f\"question 길이 최소:    {np.min(train_question_counts):4d}\")\n",
    "print(f\"question 길이 평균:    {np.mean(train_question_counts):7.2f}\")\n",
    "print(f\"question 길이 표준편차: {np.std(train_question_counts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(train_question_counts, 25)\n",
    "percentile50 = np.percentile(train_question_counts, 50)\n",
    "percentile75 = np.percentile(train_question_counts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"question 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"question 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"question 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"question IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"question MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(train_question_counts, labels=['token counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token count\n",
    "train_context_counts = [len(context) for context in contexts]\n",
    "train_context_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_context_counts, bins=900, range=[100, 1000], facecolor='r', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Count of context')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of context')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of context')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"context 길이 최대:    {np.max(train_context_counts):4d}\")\n",
    "print(f\"context 길이 최소:    {np.min(train_context_counts):4d}\")\n",
    "print(f\"context 길이 평균:    {np.mean(train_context_counts):7.2f}\")\n",
    "print(f\"context 길이 표준편차: {np.std(train_context_counts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(train_context_counts, 25)\n",
    "percentile50 = np.percentile(train_context_counts, 50)\n",
    "percentile75 = np.percentile(train_context_counts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"context 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"context 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"context 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"context IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"context MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(train_context_counts, labels=['token counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 대답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token count\n",
    "train_answer_starts = token_starts\n",
    "train_answer_starts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_answer_starts, bins=500, range=[0, 500], facecolor='g', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Count of answer')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of answer')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of answer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"answer 위치 최대:    {np.max(train_answer_starts):4d}\")\n",
    "print(f\"answer 위치 최소:    {np.min(train_answer_starts):4d}\")\n",
    "print(f\"answer 위치 평균:    {np.mean(train_answer_starts):7.2f}\")\n",
    "print(f\"answer 위치 표준편차: {np.std(train_answer_starts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(train_answer_starts, 25)\n",
    "percentile50 = np.percentile(train_answer_starts, 50)\n",
    "percentile75 = np.percentile(train_answer_starts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"answer 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"answer 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"answer 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"answer IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"answer MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(train_answer_starts, labels=['token counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train documents\n",
    "documents = []\n",
    "\n",
    "# 전체 데이터에서 title, context, question 문장을 모두 추출합니다. \n",
    "for data in tqdm(train_json[\"data\"]):\n",
    "    title = data[\"title\"]\n",
    "    documents.append(title)\n",
    "    for paragraph in data[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        documents.append(context)\n",
    "\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            assert len(qa[\"answers\"]) == 1\n",
    "            question = qa[\"question\"]\n",
    "            documents.append(question)\n",
    "\n",
    "documents[:10]   # 그중 맨 앞 10개만 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents를 전부 이어 하나의 문장으로 만들면 이렇게 보입니다. \n",
    "\" \".join(documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud로 \" \".join(documents)를 처리해 봅니다. \n",
    "wordcloud = WordCloud(width=800, height=800, font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(\" \".join(documents))\n",
    "plt.figure(figsize=(10, 10))\n",
    "# image 출력, interpolation 이미지 시각화 옵션\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = os.path.join(data_dir, \"korquad_train.json\")\n",
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "args = Config({\n",
    "    'max_seq_length': 384,\n",
    "    'max_query_length': 64,\n",
    "})\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 데이터셋 파일을 메모리에 로딩하는 함수\n",
    "def load_data(args, filename):\n",
    "    inputs, segments, labels_start, labels_end = [], [], [], []\n",
    "\n",
    "    n_discard = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Loading ...\")):\n",
    "            data = json.loads(line)\n",
    "            token_start = data.get(\"token_start\")\n",
    "            token_end = data.get(\"token_end\")\n",
    "            question = data[\"question\"][:args.max_query_length]\n",
    "            context = data[\"context\"]\n",
    "            answer_tokens = \" \".join(context[token_start:token_end + 1])\n",
    "            context_len = args.max_seq_length - len(question) - 3\n",
    "\n",
    "            if token_end >= context_len:\n",
    "                # 최대 길이내에 token이 들어가지 않은 경우 처리하지 않음\n",
    "                n_discard += 1\n",
    "                continue\n",
    "            context = context[:context_len]\n",
    "            assert len(question) + len(context) <= args.max_seq_length - 3\n",
    "\n",
    "            tokens = ['[CLS]'] + question + ['[SEP]'] + context + ['[SEP]']\n",
    "            ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "            ids += [0] * (args.max_seq_length - len(ids))\n",
    "            inputs.append(ids)\n",
    "            segs = [0] * (len(question) + 2) + [1] * (len(context) + 1)\n",
    "            segs += [0] * (args.max_seq_length - len(segs))\n",
    "            segments.append(segs)\n",
    "            token_start += (len(question) + 2)\n",
    "            labels_start.append(token_start)\n",
    "            token_end += (len(question) + 2)\n",
    "            labels_end.append(token_end)\n",
    "    print(f'n_discard: {n_discard}')\n",
    "\n",
    "    return (np.array(inputs), np.array(segments)), (np.array(labels_start), np.array(labels_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "train_inputs, train_labels = load_data(args, train_json)\n",
    "print(f\"train_inputs: {train_inputs[0].shape}\")\n",
    "print(f\"train_inputs: {train_inputs[1].shape}\")\n",
    "print(f\"train_labels: {train_labels[0].shape}\")\n",
    "print(f\"train_labels: {train_labels[1].shape}\")\n",
    "\n",
    "# dev data load\n",
    "dev_inputs, dev_labels = load_data(args, dev_json)\n",
    "print(f\"dev_inputs: {dev_inputs[0].shape}\")\n",
    "print(f\"dev_inputs: {dev_inputs[1].shape}\")\n",
    "print(f\"dev_labels: {dev_labels[0].shape}\")\n",
    "print(f\"dev_labels: {dev_labels[1].shape}\")\n",
    "\n",
    "train_inputs[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question과 Context가 포함된 입력데이터 1번째\n",
    "train_inputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question을 0으로, Context를 1로 구분해 준 Segment 데이터 1번째\n",
    "train_inputs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer위치의 시작점과 끝점 라벨 1번째\n",
    "train_labels[0][0], train_labels[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = os.path.join(model_dir, 'bert_pretrain_32000.hdf5')\n",
    "\n",
    "model = BERT4KorQuAD(config)\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    #  pretrained model 을 로드하기 위해 먼저 모델이 생성되어 있어야 한다.\n",
    "    enc_tokens = np.random.randint(0, len(vocab), (4, 10))\n",
    "    segments = np.random.randint(0, 2, (4, 10))\n",
    "    model(enc_tokens, segments)\n",
    "    \n",
    "    # checkpoint 파일로부터 필요한 layer를 불러온다. \n",
    "    model.load_weights(os.path.join(model_dir, \"bert_pretrain_32000.hdf5\"), by_name=True)\n",
    "\n",
    "    model.summary()\n",
    "else:\n",
    "    print('NO Pretrained Model')"
   ]
  },
  {
   "source": [
    "### STEP 2. pretrained model finetune"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### STEP 3. Inference 수행"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict(model, question, context):\n",
    "    \"\"\"\n",
    "    입력에 대한 답변 생성하는 함수\n",
    "    :param model: model\n",
    "    :param question: 입력 문자열\n",
    "    :param context: 입력 문자열\n",
    "    \"\"\"\n",
    "    q_tokens = vocab.encode_as_pieces(question)[:args.max_query_length]\n",
    "    c_tokens = vocab.encode_as_pieces(context)[:args.max_seq_length - len(q_tokens) - 3]\n",
    "    tokens = ['[CLS]'] + q_tokens + ['[SEP]'] + c_tokens + ['[SEP]']\n",
    "    token_ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "    segments = [0] * (len(q_tokens) + 2) + [1] * (len(c_tokens) + 1)\n",
    "\n",
    "    y_start, y_end = model(np.array([token_ids]), np.array([segments]))\n",
    "    # print(y_start, y_end)\n",
    "    y_start_idx = K.argmax(y_start, axis=-1)[0].numpy()\n",
    "    y_end_idx = K.argmax(y_end, axis=-1)[0].numpy()\n",
    "    answer_tokens = tokens[y_start_idx:y_end_idx + 1]\n",
    "\n",
    "    return vocab.decode_pieces(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")\n",
    "\n",
    "with open(dev_json) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        question = vocab.decode_pieces(data['question'])\n",
    "        context = vocab.decode_pieces(data['context'])\n",
    "        answer = data['answer']\n",
    "        answer_predict = do_predict(model, question, context)\n",
    "        if answer in answer_predict:\n",
    "            print(i)\n",
    "            print(\"질문 : \", question)\n",
    "            print(\"지문 : \", context)\n",
    "            print(\"정답 : \", answer)\n",
    "            print(\"예측 : \", answer_predict, \"\\n\")\n",
    "        if 100 < i:\n",
    "            break"
   ]
  },
  {
   "source": [
    "### STEP 4. 학습 경과 시각화 비교"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}