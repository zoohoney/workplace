{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cosmetic-lodge",
   "metadata": {},
   "source": [
    "# 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-carrier",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-supply",
   "metadata": {},
   "source": [
    "### https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "### Song Lyrice 데이터를 다운 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-governor",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-occurrence",
   "metadata": {},
   "source": [
    "### glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요. glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informal-budapest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np         \n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-gibson",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-mortgage",
   "metadata": {},
   "source": [
    "### 앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "### preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "constitutional-trout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There must be some kind of way outta here\n",
      "Said the joker to the thief\n",
      "There's too much confusion\n",
      "I can't get no relief Business men, they drink my wine\n",
      "Plowman dig my earth\n",
      "None were level on the mind\n",
      "Nobody up at his word\n",
      "Hey, hey No reason to get excited\n",
      "The thief he kindly spoke\n",
      "There are many here among us\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sought-career",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "answering-warrior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>',\n",
       " '<start> none were level on the mind <end>',\n",
       " '<start> nobody up at his word <end>',\n",
       " '<start> hey , hey no reason to get excited <end>',\n",
       " '<start> the thief he kindly spoke <end>',\n",
       " '<start> there are many here among us <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-sponsorship",
   "metadata": {},
   "source": [
    "### 추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래가사 작사하기에 어울리지 않을수도 있겠죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-moldova",
   "metadata": {},
   "source": [
    "### 그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기를 권합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-freedom",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-herald",
   "metadata": {},
   "source": [
    "### 훈련 데이터와 평가 데이터를 분리하세요!\n",
    "\n",
    "### tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intended-taste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  65 280 ...   0   0   0]\n",
      " [  2 107   6 ...   0   0   0]\n",
      " [  2  65  16 ...   0   0   0]\n",
      " ...\n",
      " [  2 238   1 ...   0   0   0]\n",
      " [  2  10 502 ...   0   0   0]\n",
      " [  2 129  21 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7ff20a4a3190>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=16)  \n",
    "# a = [] \n",
    "# for i in tensor:\n",
    "    #if > 16:\n",
    "    # return(tensor)\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰개수를 줄여볼려고 for문을 쓸려다가 실패 하고 그냥 maxlen으로 줄여보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "express-meter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175749, 16)\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape) # 모양 보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valued-liberal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "connected-october",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  65 280  27  99 528  19  85 778  93   3   0   0   0   0]\n",
      "[ 65 280  27  99 528  19  85 778  93   3   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "#scr =x #tgt =y\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "productive-tactics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 15)\n",
      "Target Train: (140599, 15)\n",
      "Source Train: (35150, 15)\n",
      "Target Train: (35150, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sr = 12000\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,       \n",
    "                                                          test_size= 0.2,\n",
    "                                                          shuffle=True)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Train:\", enc_val.shape)\n",
    "print(\"Target Train:\", dec_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벨류 모양이 궁금해서 한번 보았다. sklearn 모델을 사용할려고 하니 기억을 더듬으면서\n",
    "# 전에 노드를 돌아보며 train, val 를 나누었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latest-stanley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((256, 15), (256, 15)), types: (tf.int32, tf.int32)> <BatchDataset shapes: ((256, 15), (256, 15)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "datasetval = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "datasetval = datasetval.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(dataset, datasetval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss 값을 받기위해서 datasetval 추가 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-vegetarian",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-mileage",
   "metadata": {},
   "source": [
    "## 모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "\n",
    "### 그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "hired-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2050\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "unauthorized-browse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 15, 7001), dtype=float32, numpy=\n",
       "array([[[ 2.37124143e-04, -1.62596174e-04, -2.73255719e-04, ...,\n",
       "         -3.84996587e-04,  2.16534769e-04,  1.32960340e-04],\n",
       "        [ 1.18104438e-03, -3.69949004e-04, -2.70370976e-04, ...,\n",
       "         -4.73939086e-04,  3.91216046e-04, -1.57012910e-04],\n",
       "        [ 1.86183874e-03, -6.89579814e-04, -2.07470424e-04, ...,\n",
       "         -4.42810095e-04,  5.05483767e-04, -5.12034632e-04],\n",
       "        ...,\n",
       "        [-1.71579048e-03, -1.93317293e-03,  6.37628196e-04, ...,\n",
       "          1.74744506e-04,  1.73397973e-04, -1.88075611e-03],\n",
       "        [-1.48868409e-03, -1.65307370e-03,  7.63345684e-04, ...,\n",
       "         -4.00342047e-04, -3.27407179e-04, -2.47077737e-03],\n",
       "        [-1.42174447e-03, -1.69122184e-03,  7.06545019e-04, ...,\n",
       "         -2.66473857e-04, -6.72848953e-04, -3.22716194e-03]],\n",
       "\n",
       "       [[-3.33154923e-04,  9.53148701e-05, -1.62154814e-04, ...,\n",
       "         -1.95332068e-05,  3.91133333e-04, -3.64100706e-04],\n",
       "        [-3.76414682e-04,  6.01103064e-04, -1.36021728e-04, ...,\n",
       "          3.70661874e-04,  2.35838277e-04, -9.04536224e-04],\n",
       "        [-3.61827930e-04,  9.95806418e-04, -7.27243023e-04, ...,\n",
       "          4.20669909e-04,  6.47808920e-05, -1.03285816e-03],\n",
       "        ...,\n",
       "        [-2.23791599e-03,  3.65527143e-04, -9.18677833e-04, ...,\n",
       "         -1.88600551e-03, -1.70271844e-03,  3.16306506e-03],\n",
       "        [-2.52130185e-03,  1.03664997e-05, -7.19173055e-04, ...,\n",
       "         -2.53697927e-03, -1.82392064e-03,  3.74480849e-03],\n",
       "        [-2.80536362e-03, -3.92765476e-04, -5.25485491e-04, ...,\n",
       "         -3.13179544e-03, -1.90626387e-03,  4.22370574e-03]],\n",
       "\n",
       "       [[-3.33154923e-04,  9.53148701e-05, -1.62154814e-04, ...,\n",
       "         -1.95332068e-05,  3.91133333e-04, -3.64100706e-04],\n",
       "        [-9.76029201e-04,  3.90851255e-06, -2.59189081e-04, ...,\n",
       "         -2.58090382e-04,  9.44582571e-04, -5.24772098e-04],\n",
       "        [-1.34055875e-03,  9.52349656e-05, -2.57810258e-04, ...,\n",
       "         -9.51489084e-04,  1.26389775e-03, -7.08247477e-04],\n",
       "        ...,\n",
       "        [-1.72732992e-03,  1.40959374e-03, -3.12820484e-04, ...,\n",
       "         -2.01673550e-03, -1.08058797e-03,  2.04006745e-03],\n",
       "        [-1.96378352e-03,  1.14076224e-03, -1.65545338e-04, ...,\n",
       "         -2.46829935e-03, -1.56818691e-03,  2.89432029e-03],\n",
       "        [-2.21334770e-03,  7.77788169e-04, -2.10291710e-05, ...,\n",
       "         -2.95073912e-03, -1.94916571e-03,  3.62044573e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.33154923e-04,  9.53148701e-05, -1.62154814e-04, ...,\n",
       "         -1.95332068e-05,  3.91133333e-04, -3.64100706e-04],\n",
       "        [-2.29015386e-05, -1.49291591e-04, -1.20491604e-04, ...,\n",
       "         -1.43625861e-04,  4.94105741e-04, -8.74729885e-04],\n",
       "        [ 3.36008845e-04, -4.01809812e-04,  1.20041390e-04, ...,\n",
       "         -2.37315864e-04,  3.76030832e-04, -1.16455567e-03],\n",
       "        ...,\n",
       "        [ 3.10104550e-03,  7.01819314e-04,  1.95573014e-03, ...,\n",
       "         -2.05434300e-03,  6.09060749e-04,  1.21025085e-04],\n",
       "        [ 2.52729910e-03,  8.79277650e-04,  1.54845905e-03, ...,\n",
       "         -1.76442671e-03,  2.87500967e-04,  4.44571022e-04],\n",
       "        [ 1.94604765e-03,  9.90116270e-04,  1.27703452e-03, ...,\n",
       "         -1.90756517e-03, -2.63991999e-04,  1.29535852e-03]],\n",
       "\n",
       "       [[-3.33154923e-04,  9.53148701e-05, -1.62154814e-04, ...,\n",
       "         -1.95332068e-05,  3.91133333e-04, -3.64100706e-04],\n",
       "        [-2.62440968e-04,  9.24154592e-05, -3.57778685e-04, ...,\n",
       "         -1.57316652e-04,  5.61357592e-04, -6.67566899e-04],\n",
       "        [-6.12380216e-04,  3.86903237e-04, -4.74768138e-04, ...,\n",
       "         -3.87343520e-04,  6.59576443e-04, -6.92420348e-04],\n",
       "        ...,\n",
       "        [-2.87402654e-03, -9.08166810e-04, -1.51744462e-03, ...,\n",
       "         -3.45120090e-03, -1.68110570e-03,  4.63292422e-03],\n",
       "        [-3.08889220e-03, -1.29366515e-03, -1.16467383e-03, ...,\n",
       "         -3.95373767e-03, -1.77355821e-03,  5.01803728e-03],\n",
       "        [-3.30670527e-03, -1.68893381e-03, -8.34726205e-04, ...,\n",
       "         -4.39946074e-03, -1.82924315e-03,  5.32427663e-03]],\n",
       "\n",
       "       [[-3.33154923e-04,  9.53148701e-05, -1.62154814e-04, ...,\n",
       "         -1.95332068e-05,  3.91133333e-04, -3.64100706e-04],\n",
       "        [-1.12874535e-04,  2.67448020e-04, -5.68042218e-04, ...,\n",
       "          2.25954631e-04,  7.95977656e-04, -3.59662197e-04],\n",
       "        [-5.34467050e-04,  4.02388716e-04, -1.17585144e-03, ...,\n",
       "          6.91260153e-04,  9.46527522e-04, -1.42579185e-04],\n",
       "        ...,\n",
       "        [-3.66239808e-03, -1.91727874e-03, -2.95046812e-05, ...,\n",
       "         -4.77634370e-03, -1.54380430e-03,  5.71185211e-03],\n",
       "        [-3.88654438e-03, -2.30477960e-03,  1.42593999e-04, ...,\n",
       "         -5.15264273e-03, -1.59871147e-03,  5.89554384e-03],\n",
       "        [-4.08479432e-03, -2.66902940e-03,  3.03881068e-04, ...,\n",
       "         -5.47322072e-03, -1.63357588e-03,  6.04041666e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "incident-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      multiple                  3584512   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                multiple                  21016600  \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                multiple                  33628200  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  14359051  \n",
      "=================================================================\n",
      "Total params: 72,588,363\n",
      "Trainable params: 72,588,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cardiac-insertion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 3.1993 - val_loss: 2.8130\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 2.6504 - val_loss: 2.5594\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 2.3604 - val_loss: 2.3894\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 2.0762 - val_loss: 2.2594\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 1.7893 - val_loss: 2.1588\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 1.5178 - val_loss: 2.0933\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 1.2825 - val_loss: 2.0632\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 1.1024 - val_loss: 2.0664\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 0.9848 - val_loss: 2.0834\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 193s 352ms/step - loss: 0.9206 - val_loss: 2.1107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff15c5eb190>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, validation_data = datasetval,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "encouraging-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit 부분에 validation_data 값을 입력 하는 부분에서 노드들을 다시 돌아보고 구글링을 해봐도 도저히 답을 찾을수가 없었다.\n",
    "# 그리고 마침 들어와있는 해선님에게 도움을 요청하니 TF홈페이지에서 찾아보셨고 저에게 힌트를 줘서 많은 도움이 되었다.\n",
    "# validation_data = 256 ㅡ>512 변경 \n",
    "# hidden_size = 1024 ㅡ> 2050 변경 \n",
    "# 변경전에는 val_loss 2.3xxx 나와 변경후 2.11 값을 얻었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "irish-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "municipal-bermuda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so much <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-office",
   "metadata": {},
   "source": [
    "## 프로젝트를 마치며..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-recognition",
   "metadata": {},
   "source": [
    "### 이번프로젝트는 문제부분이 dataset val 정의하는거랑 model.fit 에서 validation_data 값을 정의 하는부분이 어려웠다. 다행이 datasetval부분은 전 익스 노드를 보면서 상기 시키며 수정 할수있었고 validation_data 값은 조금많이 힘들었지만 팀원도움으로 잘해결할수있었다. \n",
    "\n",
    "### 이번에는 토큰을 줄이는 코드를 짜볼려고 이것저것 해보았지만... 이번에는 그냥 넘어가기루 했다.. 나에게는 아직 많은 과제들이 기다리고있으니... ^^ \n",
    "\n",
    "### 다음번에는 모든 권고사항이던지 루브릭에 맞추어 더 발전해야겠다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
